<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Learning on sysid blog</title>
    <link>/blog/learning/</link>
    <description>Recent content in Learning on sysid blog</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <copyright>Copyright © 2022, sysid.</copyright>
    <lastBuildDate>Sun, 10 Mar 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="/blog/learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Anki, Markdown, and the Power of inka2</title>
      <link>/anki-markdown-and-the-power-of-inka2/</link>
      <pubDate>Sun, 10 Mar 2024 00:00:00 +0000</pubDate>
      <guid>/anki-markdown-and-the-power-of-inka2/</guid>
      <description>&lt;h3 id=&#34;streamlining-knowledge-management-anki-markdown-and-the-power-of-inka2&#34;&gt;Streamlining Knowledge Management: Anki, Markdown, and the Power of inka2&lt;/h3&gt;&#xA;&lt;h4 id=&#34;introduction&#34;&gt;Introduction&lt;/h4&gt;&#xA;&lt;p&gt;Acquiring and keeping knowledge is an uphill battle.&#xA;Luckily the digital age has awarded us two powerful tools: Anki for spaced repetition learning, and Markdown for its straightforward syntax in note-taking.&#xA;Bridging these worlds is &lt;a href=&#34;https://github.com/sysid/inka2&#34;&gt;&lt;strong&gt;inka2&lt;/strong&gt;&lt;/a&gt; a tool designed to transform Markdown notes into Anki flashcards with seamless efficiency.&lt;/p&gt;&#xA;&lt;h4 id=&#34;why-markdown-for-knowledge-management&#34;&gt;Why Markdown for Knowledge Management?&lt;/h4&gt;&#xA;&lt;p&gt;Markdown stands as the go-to format for many developers and writers, praised for its simplicity and versatility.&#xA;It allows for quick note-taking, organizing thoughts, and even documenting complex code snippets without breaking a sweat.&#xA;Coupled with a text editor like Vim, managing knowledge becomes a streamlined process, minimizing distractions and maximizing productivity.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reboot</title>
      <link>/reboot/</link>
      <pubDate>Mon, 26 Dec 2022 08:25:13 +0100</pubDate>
      <guid>/reboot/</guid>
      <description>&lt;figure class=&#34;center&#34;&gt;&lt;img src=&#34;/reboot/success.jpg&#34; width=&#34;100%&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Success is not always a straight line.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;h2 id=&#34;blog-requirements&#34;&gt;Blog Requirements&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;readability, no noise&lt;/li&gt;&#xA;&lt;li&gt;professional math support, i.e. Latex rendering&lt;/li&gt;&#xA;&lt;li&gt;swift build times&lt;/li&gt;&#xA;&lt;li&gt;robust technology&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;switch-to-gatsby&#34;&gt;Switch to Gatsby&lt;/h2&gt;&#xA;&lt;p&gt;End of 2018 I switched from Hugo to Gatsby because why not. Gatsby was the new-kid-on-the-block and it is based on React and&#xA;GraphQL, which I wanted to learn anyway. At the time I could tick above boxes, especially because my blog was&#xA;tiny and I was dealing with React in other projects.&lt;/p&gt;</description>
    </item>
    <item>
      <title>BMW I4 and linear dynamic Truchet tilings</title>
      <link>/bmw-i4-and-linear-dynamic-truchet-tilings/</link>
      <pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/bmw-i4-and-linear-dynamic-truchet-tilings/</guid>
      <description>&lt;figure class=&#34;center&#34;&gt;&lt;img src=&#34;/bmw-i4-and-linear-dynamic-truchet-tilings/i4.png&#34; width=&#34;100%&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;Isn&amp;rsquo;t this BMW I4 a beauty? The perfect motivation to start a journey into a little optimization problem:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;How to manipulate traditional Truchet tiles so that the resulting tiling resembles the Ultimate Driving Machine?&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Truchet tiles are a well known niche in math because they form a foundation for a lot of interesting questions.&#xA;The basic theory: You have four simple tiles:&lt;/p&gt;&#xA;&lt;figure class=&#34;center&#34;&gt;&lt;img src=&#34;/bmw-i4-and-linear-dynamic-truchet-tilings/truchet_tiles.png&#34; width=&#34;100%&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;Since every tile is 50% black and 50% white a resulting tiling can only resemble a grey area if viewed from afar.&#xA;In order to depict images it is necessary to manipulate the proportion of black vs. white.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Energy Forecast for a full-scale Vehicle Plant</title>
      <link>/energy-forecast-for-a-full-scale-vehicle-plant/</link>
      <pubDate>Sun, 28 May 2017 22:12:03 +0000</pubDate>
      <guid>/energy-forecast-for-a-full-scale-vehicle-plant/</guid>
      <description>&lt;figure class=&#34;center&#34;&gt;&lt;img src=&#34;/energy-forecast-for-a-full-scale-vehicle-plant/plant_lpz.png&#34; width=&#34;100%&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;h1 id=&#34;energy-forecast-for-a-full-scale-vehicle-plant&#34;&gt;Energy Forecast for a full scale Vehicle Plant&lt;/h1&gt;&#xA;&lt;p&gt;Energy forecasting is based on time series analysis.&#xA;There are many techniques for analysing and forecasting time series, e.g. ARIMA, linear regression and deep learning.&#xA;To tackle the challenge at hand a linear regression will be the benchmark model aganst which deep learning models will be tested. In particular a multi layer perceptron (MLP) and recurrent&#xA;neural network (RNN), i.e.  Long-Short Time Memory (LSTM) model will be applied.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning Journey</title>
      <link>/machine-learning-journey/</link>
      <pubDate>Mon, 23 Jan 2017 22:12:03 +0000</pubDate>
      <guid>/machine-learning-journey/</guid>
      <description>&lt;h1 id=&#34;cheat-sheet&#34;&gt;Cheat Sheet&lt;/h1&gt;&#xA;&lt;h5 id=&#34;general-explanations&#34;&gt;General Explanations:&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;embeddings: a way to translate multidimensional input into fixed length log dimensional representations: lookup the integer index of the object and look it up in a corresponding matrix wich holds the low-dim representation. If no embeddings are used, the input has to be one-hot-encoded wich yields huge matrices&lt;/li&gt;&#xA;&lt;li&gt;KFold Cross Validation:&#xA;The purpose of cross-validation is model checking, not model building.&#xA;Once we have used cross-validation to select the better performing model, we train that model&#xA;(whether it be the linear regression or the neural network) on all the data.&#xA;We don&amp;rsquo;t use the actual model instances we trained during cross-validation for our final predictive model.&lt;/li&gt;&#xA;&lt;li&gt;A dense layer in a multilayer perceptron (MLP) is a lot more feature intensive than a convolutional layer. People use convolutional nets with subsampling precisely because they get to aggressively prune the features they’re computing.&lt;/li&gt;&#xA;&lt;li&gt;in NNs rarely occur local minima due to vast parameter space (probability not to get better in ayn dimension is miniscule)&lt;/li&gt;&#xA;&lt;li&gt;the fast majority of space of a loss function in NN is all saddlepoints&lt;/li&gt;&#xA;&lt;li&gt;one training cycle for the entire dataset is called epoch, i.e. the algorithm sees the ENTIRE dataset&lt;/li&gt;&#xA;&lt;li&gt;iteration: every time a batch is passed through the NN (forward + backward pass)&lt;/li&gt;&#xA;&lt;li&gt;Latent factors = features of embeddings (used in Collaborative Filtering)&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/&#34;&gt;Bias-Variance Tradeoff in Machine Learning&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Softmax vs Sigmoid: All the softmax units in a layer are constrained to add up to 1, whereas sigmoid units don&amp;rsquo;t have this &amp;rsquo;lateral&amp;rsquo; constraint.&#xA;If every example can be associated with multiple labels, you need to use a sigmoid output layer that learns to predict &amp;ldquo;yes&amp;rdquo; or &amp;ldquo;no&amp;rdquo; for each individual label. If the classes are disjoint, i.e. each example can only belong to one class, you should use a softmax output layer to incorporate this constraint.&lt;/li&gt;&#xA;&lt;li&gt;Do not forget to fine tune your network architecture and your learning rate. If you have more data, a complex network is preferable. According to one important deep learning theorem, the local minima are very close to the global minimum for very deep neural networks.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;pre-process your data by making sure each dimension has 0 mean and unit variance. This should always be the case with data your are feeding to a NN, unless you have strong, well-understood reasons not to do it.&#xA;A simple MLP will never cause gradient explosion if your data is correctly preprocessed.&lt;/li&gt;&#xA;&lt;li&gt;Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do.&lt;/li&gt;&#xA;&lt;li&gt;However, it can make sense to scale sparse inputs, especially if features are on different scales.&lt;/li&gt;&#xA;&lt;li&gt;MaxAbsScaler and maxabs_scale were specifically designed for scaling sparse data, and are the recommended way to go&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h5 id=&#34;underfittingoverfitting&#34;&gt;Underfitting/Overfitting&lt;/h5&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Underfitting: This describes a model that lacks the complexity to accurately capture the complexity inherent in the problem you&amp;rsquo;re trying to solve. We can recognize this when our training error is much lower than our validation error&lt;/li&gt;&#xA;&lt;li&gt;Overfitting: This describes a model that is using too many parameters and has been trained too long. Specifically, it has learned how to match your exact training images to classes, but has become so specific that it is unable to generalize to similar images. This is easily recognizable when your training set accuracy is much higher than your validation.&lt;/li&gt;&#xA;&lt;li&gt;when you start overfitting you know, that your model is complex enough to handle your data&lt;/li&gt;&#xA;&lt;li&gt;Your main focus for fighting overfitting should be the entropic capacity of your model &amp;ndash;how much information your model is allowed to store. A model that can store a lot of information has the potential to be more accurate by leveraging more features, but it is also more at risk to start storing irrelevant features. Meanwhile, a model that can only store a few features will have to focus on the most significant features found in the data, and these are more likely to be truly relevant and to generalize better.&lt;/li&gt;&#xA;&lt;li&gt;Dropout also helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation (you could say that both dropout and data augmentation tend to disrupt random correlations occuring in your data).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;Recipe:&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
