<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on </title>
    <link>https://sysid.github.io/post/index.xml</link>
    <description>Recent content in Posts on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>All rights reserved - 2016</copyright>
    <lastBuildDate>Sun, 28 May 2017 12:27:54 +0200</lastBuildDate>
    <atom:link href="https://sysid.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>energy prediction lpz</title>
      <link>https://sysid.github.io/post/energy-prediction-lpz/</link>
      <pubDate>Sun, 28 May 2017 12:27:54 +0200</pubDate>
      
      <guid>https://sysid.github.io/post/energy-prediction-lpz/</guid>
      <description>


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/prognose/plant_lpz.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;h1 id=&#34;energy-forecast-for-a-full-scale-vehicle-plant&#34;&gt;Energy Forecast for a full scale Vehicle Plant&lt;/h1&gt;

&lt;p&gt;Energy forecasting is based on time series analysis.
There are many techniques for analysing and forecasting time series, e.g. ARIMA, linear regression and deep learning.
To tackle the challenge at hand a linear regression will be the benchmark model aganst which deep learning models will be tested. In particular a multi layer perceptron (MLP) and recurrent
neural network (RNN), i.e.  Long-Short Time Memory (LSTM) model will be applied.&lt;/p&gt;

&lt;h2 id=&#34;business-domain&#34;&gt;Business Domain&lt;/h2&gt;

&lt;p&gt;Energy forecasting is a tricky challenge because many factors might influence the final energy demand of a complex system
like a large manufacturing plant. Especially when the plant employs
not only energy consumers but also energy producers like CHPs and wind farms or energy reservoirs like battery farms.
Significant factors to take into account:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;production plan&lt;/li&gt;
&lt;li&gt;CHP energy production&lt;/li&gt;
&lt;li&gt;weather, i.e. temperature, wind&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;data-preparation&#34;&gt;Data Preparation&lt;/h2&gt;

&lt;p&gt;In order to apply the described techniques the problem has to be framed as a supervised learning problem. The data at hand is an hourly measurment of energy consumption
in 2015 as well as associated production plans and weather data. This results to a multivariate time series. The variable to forecast is energy consumption for the next 48 hours.&lt;/p&gt;

&lt;p&gt;For application in a LSTM neural network with &lt;strong&gt;&lt;code&gt;tanh&lt;/code&gt;&lt;/strong&gt; non-linearity the data need to be scaled to the interval [-1,1]. Furthermore we split it into a training set (80%) and a test set (20%).&lt;/p&gt;

&lt;p&gt;For forecasting different tactics can be applied.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Linear&lt;/strong&gt; regression: the timesteps are taken as independent from the past an only dependent on the feature vector at time t=0.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MLP&lt;/strong&gt;: similar to linear regression with respect to feature preparation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LSTM&lt;/strong&gt;: the timesteps are dependent on their predecessors and therefor the see-behind window is a hyperparameter to be chosen for the model.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For this analysis the LSTM model will have two variants with regards to the lookback window:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;the entire dataset will be taken as sequence length, i.e. the LSTM context will be build over the entire time series. In Keras this results in a statfull LSTM network with batch-size 1 (online learning)..&lt;/li&gt;
&lt;li&gt;a lookback window of 14days will be taken. This allows for batch-size &amp;gt; 0 and a stateless LSTM network.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For the non RNN models also information from previous timesteps can be encoded into the feature vector by just putting the values of past timesteps as additional features into the feature vector.
Here we also use the information of the last 14 days to be consistent within our model choices.&lt;/p&gt;

&lt;p&gt;For all models the following parameters/features have been selected:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;energy consumption&lt;/li&gt;
&lt;li&gt;air temperature&lt;/li&gt;
&lt;li&gt;wind speed&lt;/li&gt;
&lt;li&gt;wind direction&lt;/li&gt;
&lt;li&gt;production plan&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This results in a feature vector for the linear models of dimension 1872:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;lookback: 14days*24h*5features&lt;/li&gt;
&lt;li&gt;lookforward: 2days*24h*4features (5th parameter is the energy and is the label in our models to be forecasted)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;timestamp-challenges&#34;&gt;Timestamp Challenges&lt;/h4&gt;

&lt;p&gt;Keeping the timestamps correct after all the data transformations is a special challenge which requires careful handling. The following diagram illustrates the topic. Left you can see the resulting dataset for a lookback window of 14days whereas on the right for a lookback window of 1hour. In order to compare results, the inverse date transformations have to take this into account.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/prognose/temporal_adjustment.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;

&lt;p&gt;We predict the entire 48 hours with one prediction in order to avoid instabilities introduced by step-by-step forecasting and then using the forecast as feature for the next forecast.&lt;/p&gt;

&lt;p&gt;For the linear regression the venerable &lt;a href=&#34;http://scikit-learn.org/stable/&#34;&gt;scikit-learn&lt;/a&gt; library is used.
For all the deep-learning &lt;a href=&#34;https://keras.io/&#34;&gt;KERAS&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;TENSORFLOW&lt;/a&gt; are the tools of choice.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/prognose/models.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;The MLP model has got 1.4 Mio parameters, so its capacity is much higher then the LSTM.
This gives already a first hint towards further optimization of model setup.&lt;/p&gt;

&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;

&lt;p&gt;Quality of forecast is measured as MSE (mean squared error). All plots show an arbitrary point in time of the test set with 14days
in the past and 2 days forecast. Every model is compared to the naive linear regression (red line).&lt;/p&gt;

&lt;h3 id=&#34;ltsm&#34;&gt;LTSM&lt;/h3&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/prognose/rnn.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;The LTSM model overall shows an MSE of 0.025 on the test set.&lt;/p&gt;

&lt;p&gt;The red box shows an outlier in the linear regression.
It seems like the linear model did not pick up a significant feature like production plan properly.
The LSTM did a better job here.&lt;/p&gt;

&lt;h3 id=&#34;mlp&#34;&gt;MLP&lt;/h3&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/prognose/mlp.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;The MLP model overall shows an MSE of 0.063 on the test set.&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Although both deep learning approaches can predict the shape of the time series well, the LSTM model exhibits higher accuracy.
Since the model capacity is much lower, this was a surprising outcome.&lt;/p&gt;

&lt;p&gt;However both deep learning approaches seem struggle to match the quality of a simple linear regression forecast.
Due to time contraints no hyperparameter or model tuning has taken place. There are many areas for potential improvements, e.g.
- detailed feature preparation, especially production plans
- exploration of more neural network configurations (number of layers, number of timesteps, number of neurons, &amp;hellip;)
- hyperparameter tuning (regularization, learning rates, optimiziers, &amp;hellip;)&lt;/p&gt;

&lt;p&gt;Thanks for reading.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>fishy affine transformation</title>
      <link>https://sysid.github.io/post/fishy-affine-transformation/</link>
      <pubDate>Mon, 13 Mar 2017 06:31:17 +0100</pubDate>
      
      <guid>https://sysid.github.io/post/fishy-affine-transformation/</guid>
      <description>

&lt;h1 id=&#34;fishy-affine-transformation&#34;&gt;Fishy Affine Transformation&lt;/h1&gt;

&lt;p&gt;While working on the kaggle competition &lt;a href=&#34;https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring&#34;&gt;https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring&lt;/a&gt; I hit the point when I wanted
to align fish based on an annotation at the fish&amp;rsquo;s head and tail, so that the fish is centered in the image, always in the same orientation
and distracting picture information is minimized. This required:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;finding the fish (thanks Nathaniel Shimoni for annotating)&lt;/li&gt;
&lt;li&gt;centering&lt;/li&gt;
&lt;li&gt;rotatating&lt;/li&gt;
&lt;li&gt;cropping&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Mathematically the challenge is to find the associated  Affine Transformation. After years of working in a managerial role my linear algebra skills are a bit rusty so I decided to
invest the weekend.&lt;/p&gt;

&lt;h3 id=&#34;affine-transformation&#34;&gt;Affine Transformation&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://mathworld.wolfram.com/AffineTransformation.html&#34;&gt;Wolfram&lt;/a&gt;: An affine transformation is any transformation that preserves collinearity (i.e., all points lying on a line initially still lie on a line after transformation) and ratios of distances (e.g., the midpoint of a line segment remains the midpoint after transformation).&lt;/p&gt;

&lt;p&gt;I decided to use &lt;a href=&#34;http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html#transformations&#34;&gt;CV2&lt;/a&gt; after hitting the wall with several other tools.
It was not the most convenient choice, but eventually it got me there. CV2 uses (2x3) transformation matrices for affine transformations so I had to adjust my 2d vectors accordingly.&lt;/p&gt;

&lt;p&gt;The reason: Homogeneous Coordinates.&lt;/p&gt;

&lt;p&gt;To combine rotation and translation in one operation one extra dimension is needed more than the model requires.
For planar things this is 3 components and for spatial things this is 4 components.
The operators take 3 components and return 3 components requiring 3x3 matrices.&lt;/p&gt;

&lt;p&gt;Using vector algebra with numpy requires some extra consideration but is possible. Basically a (2,) matrix represented the 2-dim vectors. Small letters
denoted vector variables and caps matrices.&lt;/p&gt;

&lt;h2 id=&#34;1-finding-the-fish&#34;&gt;1. Finding the Fish&lt;/h2&gt;

&lt;p&gt;I used the annotations from labels produced by Nathaniel Shimoni and published on Kaggle (thanks for the great work!).&lt;/p&gt;

&lt;p&gt;Using only fish with head and tail annotated, it was possible to get the vector representation of a fish as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;p_heads = np.array((img_data[&#39;annotations&#39;][0][&#39;x&#39;], img_data[&#39;annotations&#39;][0][&#39;y&#39;]))
p_tails = np.array((img_data[&#39;annotations&#39;][1][&#39;x&#39;], img_data[&#39;annotations&#39;][1][&#39;y&#39;]))
p_middle = (p_heads + p_tails)/2
v_fish = p_heads - p_tails
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-centering&#34;&gt;2. Centering&lt;/h2&gt;

&lt;p&gt;Centering fish is a basic translation in the 2-dim space.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    # translate to center of img
    img_center = np.array([img_height/2, img_width/2])
    t = img_center - p_middle  # translation vector
    t = np.reshape(t, (2,1))  # generate the 2x3 affine transformation matrix
    T = np.concatenate((np.identity(2), t), axis=1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The respective transformation matrix is:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/fishy-affine-transformation-translation.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;h2 id=&#34;3-rotating&#34;&gt;3. Rotating&lt;/h2&gt;

&lt;p&gt;First I needed to find the angle for rotation. I wanted to have the fish oriented parallel to the x-axis with the head always being on the right. The dot-product of two vectors provides the
angle in between, so I had to &amp;lsquo;dot-product&amp;rsquo; my fish vector with the x-axis:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def unit_vector(vector):
    &amp;quot;&amp;quot;&amp;quot; Returns the unit vector of the vector.&amp;quot;&amp;quot;&amp;quot;
    return vector / np.linalg.norm(vector)

def angle_between(v1, v2):
    &amp;quot;&amp;quot;&amp;quot; Returns the angle in radians between vectors &#39;v1&#39; and &#39;v2&#39;::

            &amp;gt;&amp;gt;&amp;gt; angle_between((1, 0, 0), (0, 1, 0))
            1.5707963267948966
            &amp;gt;&amp;gt;&amp;gt; angle_between((1, 0, 0), (1, 0, 0))
            0.0
            &amp;gt;&amp;gt;&amp;gt; angle_between((1, 0, 0), (-1, 0, 0))
            3.141592653589793
    &amp;quot;&amp;quot;&amp;quot;
    v1_u = unit_vector(v1)
    v2_u = unit_vector(v2)
    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))

angle = np.rad2deg(angle_between((1, 0), v_fish))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Conveniently CV2 provides a function to find the necessary transformation matrix (cv2.getRotationMatrix2D).&lt;/p&gt;

&lt;p&gt;A challenge was to find out, that the rotation angle returned always is between 0-180°, so the following conditional differentiation was necessary
(rotation counter clockwise vs clockwise). It basically differentiates between the case that the head is above or below the tail:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    # get the Affine transformation matrix
    if p_heads[1] &amp;gt; p_tails[1]:  # head is above tail
        M = cv2.getRotationMatrix2D((p_middle[0], p_middle[1]), angle, 1)
    else:
        M = cv2.getRotationMatrix2D((p_middle[0], p_middle[1]), -angle, 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;putting-it-all-together&#34;&gt;Putting it all together&lt;/h2&gt;

&lt;p&gt;Getting the resulting transformation from a translation and rotation mathematically translates to a matrix product and applying the resulting
transformation matrix to the fish vector. To make the multiplication of a 2x3 tranlation matrix and a 2x3 rotation matrix possible the
following steps were necesary (combination of two affine transformations):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;allocate A1, A2, R matrices, all 3x3 identity matrices (eyes)&lt;/li&gt;
&lt;li&gt;replace the top part of A1 and A2 with the transformation matrices T and M&lt;/li&gt;
&lt;li&gt;get the resulting transformation (matrix product)&lt;/li&gt;
&lt;li&gt;return the first two rows of R&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So RR was my final transformation matrix.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    # compinte affine transform: make them 3x3
    # http://stackoverflow.com/questions/13557066/built-in-function-to-combine-affine-transforms-in-opencv
    A1 = np.identity(3)
    A2 = np.identity(3)
    R = np.identity(3)
    A1[:2] = T
    A2[:2] = M
    R = A1@A2
    RR = R[:2]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Getting the transformed image is now straightforward:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    dst = cv2.warpAffine(img, RR, (img_height, img_width))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The nice thing with this approach is that once you have got the final transformation matrix, all other points of interest can be transformed by this matrix,
e.g. the head and tail annotations are transformed by the same matrix.&lt;/p&gt;

&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;

&lt;p&gt;The blue point marks the head and the red point the tail. You can see the fish positioned arbitrarily in the image.
With the Affine Transformation the fish will be extracted and aligned.
The result is being displayed in the left upper corner.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/fishy-affine-transformation-result.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;With this technique I was able to align my fish and feed it into my machine learning models.&lt;/p&gt;

&lt;p&gt;Thanks for reading.&lt;/p&gt;

&lt;h5 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h5&gt;

&lt;p&gt;I use &lt;a href=&#34;http://stackoverflow.com/&#34;&gt;http://stackoverflow.com/&lt;/a&gt; a lot. Not every source is quoted properly.&lt;br /&gt;
Other sources:&lt;br /&gt;
&lt;a href=&#34;https://www.kaggle.com/qiubit/the-nature-conservancy-fisheries-monitoring/crop-fish&#34;&gt;https://www.kaggle.com/qiubit/the-nature-conservancy-fisheries-monitoring/crop-fish&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>energy consumption prediction with prophet</title>
      <link>https://sysid.github.io/post/energy-consumption-prediction-with-prophet/</link>
      <pubDate>Tue, 07 Mar 2017 21:53:53 +0100</pubDate>
      
      <guid>https://sysid.github.io/post/energy-consumption-prediction-with-prophet/</guid>
      <description>

&lt;h1 id=&#34;energy-consumption-prediction-with-facebook-s-prophet-library&#34;&gt;Energy Consumption Prediction with Facebook&amp;rsquo;s Prophet library&lt;/h1&gt;

&lt;p&gt;I already introduced &lt;a href=&#34;https://facebookincubator.github.io/prophet/&#34;&gt;Prophet&lt;/a&gt; as a time-series prediction tool which has a
exceptionally well designed user interface and is easy to use for simple cases while delivering impressive results
(&lt;a href=&#34;https://sysid.github.io/post/be-a-prophet-for-airline-data/&#34;&gt;see airline use case&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Today I will use &lt;a href=&#34;https://facebookincubator.github.io/prophet/&#34;&gt;Prophet&lt;/a&gt; for predicting energy consumption in Germany based on real data. Recently I covered the &lt;a href=&#34;https://sysid.github.io/post/energy-consumption-prediction/&#34;&gt;same use
case with Recurrent Neural Networks&lt;/a&gt; so this will serve as comparison of two different prediction approaches.&lt;/p&gt;

&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;

&lt;p&gt;Raw data is the hourly energy consumption of Germany from 2014 to end of 2016 and then further predicted up until 2018.
This timeseries from 2014-2018 will then in total be used as basis for predition into 2019.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumptionProphet/consumptionProphet1.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;

&lt;p&gt;Again it is surprising how user friendly Facebook designed the library interface. You don&amp;rsquo;t really need to have any
programming skills in order to use most of the framework out of the box:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m = Prophet()
m.fit(de)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;prediction&#34;&gt;Prediction&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;days = 365
future = m.make_future_dataframe(periods=24*days, freq=&#39;H&#39;)
forecast = m.predict(future)
m.plot(forecast)
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumptionProphet/consumptionProphet2.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;You can see high variance in the forecast due to day and night differences of 40000 GW. It would be a straightforward exercise
to narrow the variance by eliminating day-night differences and only look at averages or top figures.&lt;/p&gt;

&lt;p&gt;When looking at the components of the intrapolated curve you can nicely identify the trends in energy consumption on weekends
as well as during summer months and the christmas break. Also the overall decreasing trend in power consumption has been picked up correctly. The little peak in 2016 does not seem to be statically significant. However, this probably needs further investigation.&lt;/p&gt;

&lt;p&gt;The nice thing is: Prophet gives you this decomposition &amp;lsquo;for free&amp;rsquo;.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumptionProphet/consumptionProphet3.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;Not only fits the model the real seasonal trends quite well, it extrapoloates them into the future as prediction. The quality of
the prediction has not been assessed, but in this analysis I am only interested in picking up the time dependencies and trends over an extended period.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Compared to to time series prediction with Recurrent Neuronal Networks the approach is much simpler and time effective. At least
for this particular well behaved use case the results are good in both cases.&lt;/p&gt;

&lt;p&gt;Certainly this paper is only a very rough qualitätive proof of concept, but it is promising enough to incorporate  &lt;a href=&#34;https://facebookincubator.github.io/prophet/&#34;&gt;Prophet&lt;/a&gt; into my toolbelt for time series forecasting.&lt;/p&gt;

&lt;p&gt;Thanks for reading.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>energy consumption prediction</title>
      <link>https://sysid.github.io/post/energy-consumption-prediction/</link>
      <pubDate>Sun, 05 Mar 2017 19:39:50 +0100</pubDate>
      
      <guid>https://sysid.github.io/post/energy-consumption-prediction/</guid>
      <description>

&lt;h1 id=&#34;energy-consumption-prediction-with-recurrent-neural-networks&#34;&gt;Energy Consumption Prediction with Recurrent Neural Networks&lt;/h1&gt;

&lt;p&gt;We look at 52 European energy markets and their power consumption patterns.&lt;/p&gt;

&lt;p&gt;The forecast technology will be based on recurrent neural networks.
While there are many other, also simpler techniques (e.g. ARIMA, Facebook&amp;rsquo;s Prophet, time-shifting) this is an attempt to harness machine learning capabilities to predict future consumption patterns.&lt;/p&gt;

&lt;p&gt;The data consists of hourly values of country/region power consumption of 52 countries over the course from 1.1.2014 - 31.12.2018,
which gives us 43000 data points per country. Of course the data consists only of
real world data up to 2016 and then forcast data from a commercial data provider up until end of 2018.&lt;/p&gt;

&lt;p&gt;For our purposes we assume the entire time series as real data and try to predict time intervals with machine learning techniques.&lt;/p&gt;

&lt;h4 id=&#34;energy-consumption-germany&#34;&gt;Energy Consumption Germany&lt;/h4&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/consumption_DE.png&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;br&gt;
The data is generally quite regular and periodic, so this seems to be an adequate use case for RNN prediction.&lt;/p&gt;

&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;

&lt;p&gt;The problem we are facing is Multivariate time series regression.&lt;/p&gt;

&lt;p&gt;In order to run the code, we make sure that the data matrix is a numpy array of size
&lt;strong&gt;&lt;code&gt;T x N&lt;/code&gt;&lt;/strong&gt; where &lt;strong&gt;&lt;code&gt;T&lt;/code&gt;&lt;/strong&gt; is the length of time series and &lt;strong&gt;&lt;code&gt;N&lt;/code&gt;&lt;/strong&gt; is the number of them.&lt;/p&gt;

&lt;p&gt;Technically, with RNNs we should pass the entire history of time series and RNNs should be able to capture
the patterns over long period of time.&lt;/p&gt;

&lt;p&gt;Another approach is to break long sequences into smaller pieces. The size of window determines the longest
patterns in time that can possibly be captured by RNN. You need to select the window size according to the
expected length of temporal patterns.&lt;/p&gt;

&lt;p&gt;For our analysis we set the period length at 80 datapoints which is equivalent to an 80hour time forecast.&lt;/p&gt;

&lt;p&gt;So the entire &amp;lsquo;history&amp;rsquo; for our RNN consists of 80 hours of energy consumption.&lt;/p&gt;

&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;

&lt;p&gt;I tested three different models based on Gated Recurrent Units (GRU) as RNN units.&lt;/p&gt;

&lt;p&gt;The dataset, a &lt;strong&gt;&lt;code&gt;43000x52&lt;/code&gt;&lt;/strong&gt; matrix, was sequenced into a &lt;strong&gt;&lt;code&gt;464x80x52&lt;/code&gt;&lt;/strong&gt; tensor for training and a &lt;strong&gt;&lt;code&gt;82x80x52&lt;/code&gt;&lt;/strong&gt; tensor for testing.
This gives us 82 potential time periods of 80 hours per market for testing.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;&lt;code&gt;464x80x52&lt;/code&gt;&lt;/strong&gt; tensor is equivalent to 464 different timeseries per market, each of length 80.&lt;/p&gt;

&lt;p&gt;The model has been trained on the training set with &lt;strong&gt;&lt;code&gt;100&lt;/code&gt;&lt;/strong&gt; epochs and a &lt;strong&gt;&lt;code&gt;batch_size of 64&lt;/code&gt;&lt;/strong&gt;. Training took about 2 minutes per model on a NVIDIA GTX780.&lt;/p&gt;

&lt;p&gt;To test the accuracy I will here only present a visual comparision between the predicted curve and the real data. To this end a time sequence from the test set
and a market has been chosen randomly and plotted.&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;h3 id=&#34;model-1&#34;&gt;Model 1&lt;/h3&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/model1.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/model1_i10_m08.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;h3 id=&#34;model-2&#34;&gt;Model 2&lt;/h3&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/model2.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/model2_i27_m50.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;h3 id=&#34;model-3&#34;&gt;Model 3&lt;/h3&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/model3.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/model3_i46_m50.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;I did not bother to retransform the axis scales to the original units MW and Date because here I am only interested in a qualitative asssessment of
forecast quality.&lt;/p&gt;

&lt;p&gt;Surprising for me was to see, that all three models more or less deliver similar good results in projecting the time dependency of the energy consumption.&lt;/p&gt;

&lt;p&gt;This is remarkable since there has been no hyperparamter tuning at all. The models were fit &amp;lsquo;out of the box&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;Since this result has been achieved with very little work, using RNNs for this kind of time series prediction seems to be effective.&lt;/p&gt;

&lt;p&gt;Thanks for reading.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Facebook&#39;s Prophet forecasting library</title>
      <link>https://sysid.github.io/post/be-a-prophet-for-airline-data/</link>
      <pubDate>Sun, 05 Mar 2017 17:16:36 +0100</pubDate>
      
      <guid>https://sysid.github.io/post/be-a-prophet-for-airline-data/</guid>
      <description>

&lt;h1 id=&#34;be-a-prophet-for-1960-s-airline-data&#34;&gt;Be a prophet for 1960&amp;rsquo;s airline data&lt;/h1&gt;

&lt;p&gt;Facebook released a forecasting library for business timeseries, called &lt;a href=&#34;https://facebookincubator.github.io/prophet/&#34;&gt;Prophet&lt;/a&gt;. It&amp;rsquo;s designed
to work similar to the phantastic &lt;a href=&#34;phttp://scikit-learn.org/stable/&#34;&gt;scikit-learn&lt;/a&gt; library and is therefor easy to use.&lt;/p&gt;

&lt;p&gt;In order to give an example of its usefulness and effectiveness I will apply it to the good old airline passenger dataset.&lt;/p&gt;

&lt;p&gt;The input to &lt;strong&gt;Prophet&lt;/strong&gt; is always a dataframe with two columns: &lt;strong&gt;&lt;code&gt;ds&lt;/code&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;code&gt;y&lt;/code&gt;&lt;/strong&gt;. The &lt;strong&gt;&lt;code&gt;ds (datestamp)&lt;/code&gt;&lt;/strong&gt; column must contain a date or datetime (either is fine). The &lt;strong&gt;&lt;code&gt;y&lt;/code&gt;&lt;/strong&gt; column must be numeric, and represents the measurement we wish to forecast.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(DATA_HOME_DIR+&#39;/international-airline-passengers.csv&#39;,
                 sep=&#39;;&#39;,
                 names=[&#39;ds&#39;, &#39;y&#39;],
                 header=0,
                 parse_dates=[0],
                 nrows=144,
                )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since the data seems to incorporate an exponential growth trend, we preprocess it by taking the &lt;strong&gt;&lt;code&gt;log&lt;/code&gt;&lt;/strong&gt; and use the linear
trend fitting capabilities of &lt;strong&gt;Prophet&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;y&#39;] = np.log(df[&#39;y&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We only have monthly data, so certainly there will be no weekly seasonality in the date. Forecasting must take this into account and choose the right frequency for the target dates.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m = Prophet(weekly_seasonality=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can simply call the &lt;strong&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/strong&gt; method in order to fit the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m.fit(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Predictions are then made on a dataframe with a column &lt;strong&gt;&lt;code&gt;ds&lt;/code&gt;&lt;/strong&gt; containing the dates for which a prediction is to be made. You can get a suitable dataframe that extends into the future a specified number of days using the helper method &lt;strong&gt;&lt;code&gt;Prophet.make_future_dataframe&lt;/code&gt;&lt;/strong&gt;. By default it will also include the dates from the history, so we will see the model fit as well.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;future = m.make_future_dataframe(periods=36, freq=&#39;M&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/strong&gt; method will assign each row in &lt;strong&gt;&lt;code&gt;future&lt;/code&gt;&lt;/strong&gt; a predicted value which it names yhat. If you pass in historical dates, it will provide an in-sample fit. The forecast object here is a new dataframe that includes a column yhat with the forecast, as well as columns for components and uncertainty intervals.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;forecast = m.predict(future)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result is promising:

&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/airline_fit.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Prophet automatically gives you an overall trend analysis and decomposes the time series into its constituing compontents like yearly seasonality:

&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/airline_comp1.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Seeing a almost perfect linear trend confirms the original hypothesis of having a exponential growth trend.&lt;/p&gt;

&lt;p&gt;The steps in the yearly trend plot seem to be an artefact of fitting. Having a peak in July for travel numbers seem to indicate  the holiday season in the western hemisphere. However this is just an asumption since I have not been born then and are not familiar of the holiday season arrangments of the time.&lt;/p&gt;

&lt;p&gt;If you are interested in another helpful example, I refer to the excellent article of Arne: &lt;a href=&#34;https://arnesund.com/2017/02/26/using-facebook-prophet-forecasting-library-to-predict-the-weather/&#34;&gt;https://arnesund.com/2017/02/26/using-facebook-prophet-forecasting-library-to-predict-the-weather/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LSTM versus MLP for timeseries</title>
      <link>https://sysid.github.io/post/adding-problem/</link>
      <pubDate>Mon, 20 Feb 2017 07:23:42 +0100</pubDate>
      
      <guid>https://sysid.github.io/post/adding-problem/</guid>
      <description>

&lt;p&gt;Long Short Term Memory neural networks versus Multi Layer Perceptrons for time series:&lt;/p&gt;

&lt;p&gt;Playing around with RNN and LSTM for time series modelling so far resulted in disappointment. Traditional MLPs seem to perform better.
On the internet RNNs are often recommended for time-series data, but my results do not confirm this sentiment. Published examples on the internet normaly do not include a comparision
with MLP models, so I decided to analyse performance of LSTM time-series forecasting versus MLP systematically.&lt;/p&gt;

&lt;p&gt;First I needed to decided on the experimental setup and I started with a dataset which migth not seem as the natural first choice for timeseries examples: The Adding Problem.&lt;/p&gt;

&lt;p&gt;My inspiration herefor was &lt;a href=&#34;http://simaaron.github.io/Estimating-rainfall-from-weather-radar-readings-using-recurrent-neural-networks/&#34;&gt;http://simaaron.github.io/Estimating-rainfall-from-weather-radar-readings-using-recurrent-neural-networks/&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;part-1-the-adding-problem&#34;&gt;Part 1: The Adding Problem&lt;/h3&gt;

&lt;p&gt;The prediction of cumulative values from variable-length sequences of vectors with a ‘time’ component is highly reminiscent of the so-called
&lt;em&gt;Adding Problem&lt;/em&gt;
 in machine learning—a toy sequence regression task that is designed to demonstrate the power of recurrent neural networks (RNN) in learning long-term dependencies (see
 &lt;a href=&#34;http://arxiv.org/abs/1504.00941&#34;&gt;Le et al.&lt;/a&gt;
 , Sec. 4.1, for a recent example):&lt;/p&gt;

&lt;!-- 
&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/RNN_adding.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;RNN&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
 --&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/RNN_adding.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;&lt;br&gt;
Braced with this data I started my experiments to compare LSTM with MLP &lt;a href=&#34;../../nbs/adding.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Traditional MLP models seem to yield better results for this kind of sequence problem. They converge for sequence sizes &amp;gt; 50 and seem to have lower MSE.
This leaves me wondering whether the prevalent opinion on the internet on RNNs and especially LSTMs for time series data modelling seems to be misguided.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>