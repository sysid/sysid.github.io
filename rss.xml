<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>sysid blog</title>
        <link>/</link>
        <description>all things</description>
        <lastBuildDate>Mon, 11 Nov 2019 19:06:23 GMT</lastBuildDate>
        <docs>http://blogs.law.harvard.edu/tech/rss</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Machine Learning Endeavour]]></title>
            <link>/posts/2017-01-23/ml</link>
            <guid>/posts/2017-01-23/ml</guid>
            <content:encoded><![CDATA[<h1 id="cheat-sheet">Cheat Sheet</h1><h5 id="general-explanations">General Explanations:</h5><ul><li>embeddings: a way to translate multidimensional input into fixed length log dimensional representations: lookup the integer index of the object and look it up in a corresponding matrix wich holds the low-dim representation. If no embeddings are used, the input has to be one-hot-encoded wich yields huge matrices</li><li>KFold Cross Validation:
The purpose of cross-validation is model checking, not model building.
Once we have used cross-validation to select the better performing model, we train that model
(whether it be the linear regression or the neural network) on all the data.
We don’t use the actual model instances we trained during cross-validation for our final predictive model.</li><li>A dense layer in a multilayer perceptron (MLP) is a lot more feature intensive than a convolutional layer. People use convolutional nets with subsampling precisely because they get to aggressively prune the features they’re computing.</li><li>in NNs rarely occur local minima due to vast parameter space (probability not to get better in ayn dimension is miniscule)</li><li>the fast majority of space of a loss function in NN is all saddlepoints</li><li>one training cycle for the entire dataset is called epoch, i.e. the algorithm sees the ENTIRE dataset</li><li>iteration: every time a batch is passed through the NN (forward + backward pass)</li><li>Latent factors = features of embeddings (used in Collaborative Filtering)</li><li><a href="http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/">Bias-Variance Tradeoff in Machine Learning</a></li><li>Softmax vs Sigmoid: All the softmax units in a layer are constrained to add up to 1, whereas sigmoid units don’t have this “lateral” constraint.
If every example can be associated with multiple labels, you need to use a sigmoid output layer that learns to predict “yes” or “no” for each individual label. If the classes are disjoint, i.e. each example can only belong to one class, you should use a softmax output layer to incorporate this constraint.</li><li>Do not forget to fine tune your network architecture and your learning rate. If you have more data, a complex network is preferable. According to one important deep learning theorem, the local minima are very close to the global minimum for very deep neural networks.</li></ul><h5 id="preprocessing">Preprocessing</h5><ul><li>pre-process your data by making sure each dimension has 0 mean and unit variance. This should always be the case with data your are feeding to a NN, unless you have strong, well-understood reasons not to do it.
A simple MLP will never cause gradient explosion if your data is correctly preprocessed.</li><li>Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do.</li><li>However, it can make sense to scale sparse inputs, especially if features are on different scales.</li><li>MaxAbsScaler and maxabs_scale were specifically designed for scaling sparse data, and are the recommended way to go</li></ul><h5 id="underfittingoverfitting">Underfitting/Overfitting</h5><ul><li>Underfitting: This describes a model that lacks the complexity to accurately capture the complexity inherent in the problem you’re trying to solve. We can recognize this when our training error is much lower than our validation error</li><li>Overfitting: This describes a model that is using too many parameters and has been trained too long. Specifically, it has learned how to match your exact training images to classes, but has become so specific that it is unable to generalize to similar images. This is easily recognizable when your training set accuracy is much higher than your validation.</li><li>when you start overfitting you know, that your model is complex enough to handle your data</li><li>Your main focus for fighting overfitting should be the entropic capacity of your model —how much information your model is allowed to store. A model that can store a lot of information has the potential to be more accurate by leveraging more features, but it is also more at risk to start storing irrelevant features. Meanwhile, a model that can only store a few features will have to focus on the most significant features found in the data, and these are more likely to be truly relevant and to generalize better.</li><li>Dropout also helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation (you could say that both dropout and data augmentation tend to disrupt random correlations occuring in your data).</li></ul><p><strong>Recipe:</strong></p><ol><li>Add more data</li><li>Use data augmentation</li><li>Use architectures that generalize well</li><li>Add regularization</li><li>Reduce architecture complexity.</li></ol><h2 id="recommendation-and-tricks">Recommendation and Tricks</h2><h5 id="general">General</h5><ul><li>be aware of the curse of dimensionality</li><li>less parameter tend to generalize better</li><li>there is no inherent problem with using an SVM (or other regularised model such as ridge regression, LARS, Lasso, elastic net etc.) on a problem with only 120 observations but thousands of attributes, provided the regularisation parameters are tuned properly.</li><li>Rule of thumb: number of parameters &gt; number of examples = trouble</li><li>different input/pic sizes: final Dense layer does not work, all others don’t care of the input size, so to create the conv-features, you can use any size</li><li>make the first Keras layer a batchNorm layer, it does normalization for you and allows for higher learning rates</li><li>regularization you cannot do on a sample, to understand how much regularization is necessary you need the entire dataset</li><li>for kaggle use clipping to avoid the logloss problem!!!</li><li>convnet: any kind of data with consistent ordering, audio, consistent timeseries, ordered data</li><li>instead of one-hot-encoding the labels(target) we can use a cool optimizer in keras: <strong><code>loss=&#x27;sparse_categorical_crossentropy&#x27;</code></strong> takes an integer target (categorical_crossentropy takes one-hot-encoded target)</li><li>When the dataset size is limited, it seems augmenting the training labels is just as important as augmenting the training data (i.e. image perturbation)</li><li>if you deep net is not working, then use less hidden layers, until it works (simplify)</li></ul><h5 id="general-neural-networks">General Neural Networks</h5><ul><li>Unless you want to know which are the informative attributes, it is usually better to skip feature selection step and just use regularization to avoid over-fitting the data.</li><li>We no longer need to extract features when using deep learning methods as we are performing automatic feature learning. A great benefit of the approach.</li><li>functional model in keras allows adding metadata on later layers, e.g. image size after the conv-layers so that dense layers have this information to work with</li></ul><h5 id="dropout">Dropout</h5><ul><li>Today dropout starts in early layers with .1/.2 … .5 for the connected layers</li><li>Dropout eliminates information just like random forests (randomly selected new models)</li></ul><h5 id="data-augmentation">Data Augmentation</h5><ul><li>no augmentation for validation sets</li><li>vertical flippings? do you see cats on their head?</li><li>use channel augmentation</li></ul><h5 id="pseudo-labeling-semi-supervised-learning">Pseudo Labeling, Semi-Supervised Learning</h5><ul><li>One remarkably simple approach to utilizing unlabelled data is known as psuedo-labeling. Suppose we have a model that has been trained on labelled data, and the model produces reasonably well validation metrics. We can simply use this model then to make predictions on all of our unlabelled data, and then use those predictions as labels themselves. This works because while we know that our predictions are not the true labels, we have reason to suspect that they are fairly accurate. Thus, if we now train on our labelled and psuedo-labeled data we can build a more powerful model by utilizing the information in the previously unlabelled data to better learn the general structure.</li><li>One parameter to be mindful of in psuedo-labeling is the proportion of true labels and psuedo-labels in each batch. We want our psuedo-labels to enhance our understanding of the general structure by extending our existing knowledge of it. If we allow our batches to consist entirely of psuedo-labeled inputs, then our batch is no longer a valid representation of the true structure. The general rule of thumb is to have 1/4–1/3 of your batches be psuedo-labeled.</li><li>Pseudo-Labeling: ca. 30% in a batch</li></ul><h5 id="training">Training</h5><ul><li>online learning (batch size=1): network update foreach training example -&gt; quick, but can be chaotic</li><li>batch learning: save the errors across all training examples and update network at the end -&gt; more stable, typical batch size 10–100</li><li>larger batch is always better. The rule of thumbs is to have the largest possible batch your GPU can handle. The bigger your gradients are, the more accurate and smooth they will be. If you have batch_size=1, you can still converge to an optimal value, but it will be way more chaotic (much higher variance but still unbiased). And it will be way slower!</li><li>If network doesn’t fit, decrease the batch size, since most of the memory is usually consumed by the activations.</li><li>start with a very small learning rate until the loss function is better then baseline chance</li><li>Batchnorm: 10x or more improvements in training speed.
Because normalization greatly reduces the ability of a small number of outlying inputs to over-influence the training, it also tends to reduce overfitting.</li><li>Having Batch Norm added, can allow us to increase the Learning Rate, since BN will allow our activations to make sure it doesn’t go really high or really low.</li><li>use RMSprop, much faster than SGD</li><li>to continue training: just call .fit one or several times and you will be able to continue to train the model. If you want to continue the training in another process, you just have to load the weights and call model.fit()</li><li>fchollet: compiling a model does not modify its state. Weights after compilation are the same as before compilation.</li><li>keras: compiling the model does does not hurt, however, changing trainable=true does not require it since the model does not change, only the metadata</li><li>Sanity check: Overfit a tiny subset of data. try to train on a tiny portion (e.g. 20 examples) of your data and make sure you can achieve zero cost. Set regularization to zero, otherwise this can prevent from getting zero cost. Unless you pass this sanity check with a small dataset it is not worth proceeding to the full dataset. Note that it may happen that you can overfit very small dataset but still have an incorrect implementation. For instance, if your datapoints’ features are random due to some bug, then it will be possible to overfit your small training set but you will never notice any generalization when you fold it your full dataset ([source]<a href="http://cs231n.github.io/neural-networks-3/#gradcheck">http://cs231n.github.io/neural-networks-3/#gradcheck</a>)</li><li>increasing the regularization strength should increase the los</li><li>Look for correct loss at chance performance (Regularization strength zero). For example, for CIFAR-10 with a Softmax classifier initial loss should be 2.302, because we expect a diffuse probability of 0.1 for each class (since there are 10 classes), and Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302.</li><li>Validation accuracy can remain flat while the loss gets worse as long as the scores don’t cross the threshold where the predicted class changes.</li><li>Don’t let the regularization overwhelm the data. Loss function is a sum of the data loss and the regularization loss (e.g. L2 penalty on weights). Regularization loss may overwhelm the data loss, in which case the gradients will be primarily coming from the regularization term (which usually has a much simpler gradient expression).</li><li><a href="http://cs231n.github.io/neural-networks-3/#gradcheck">Traing curves</a>:<div class="document_logo__1FvRe"><img src="/static/media/learningrates.9ccd225e.jpeg" class="document_logo-plant__sUnR_" alt="MLP"/></div></li></ul><h5 id="transfer-learning">Transfer Learning</h5><ul><li>Fully connected network localizes the interesting part in transfer learning to your specific problem domain</li><li>start with the weights at the trained level, don’t randomize!</li></ul><h5 id="data-leakagemetadata">Data Leakage/Metadata</h5><ul><li>often the main data already encorporates the added metadata, so it does not improve (e.g. pic size of fisherboats in fishing competition: 8 boats the net already learned about from the pics)</li><li>metadata often is not worth the effort</li></ul><h5 id="batchnorm-batch-normalization">Batchnorm, Batch Normalization</h5><ul><li>It normalizes each layer</li><li>can be used to just normalize data at the input layer.</li><li>There are some additional steps that Batch Norm offers to make it work with SGD(the activations):<ul><li>Adds 2 more trainable parameters to each layer.<ul><li>One for multiplying the activations and set an arbitrary Standard Deviation.</li><li>The other for adding all the activations and set an arbitrary Mean.</li><li>BN (Batch Norm) doesn’t change all the weights, but only those two parameters with the activations. This makes it more stable in practice.</li></ul></li></ul></li></ul><h5 id="hyperparameters">Hyperparameters:</h5><ul><li>Hyperparameter ranges. Search for hyperparameters on log scale. learning_rate = 10 ** uniform(-6, 1). The same strategy should be used for the regularization strength. This is because learning rate and regularization strength have multiplicative effects on the training dynamics.</li><li>Prefer random search to grid search.</li></ul><h2 id="architecture">Architecture</h2><p>Rule of thumb: For a three layer network with n input and m output neurons, the hidden layer would have sqrt(n*m) neurons.</p><h3 id="number-of-hidden-layers">number of hidden layers</h3><ul><li>0 - Only capable of representing linear separable functions or decisions.</li><li>1 - Can approximate any function that contains a continuous mapping from one finite space to another.</li><li>2 - Can represent an arbitrary decision boundary to arbitrary accuracy with rational activation functions and can approximate any smooth mapping to any accuracy.</li></ul><h3 id="ensembles"><a href="http://cs231n.github.io/neural-networks-3/#gradcheck">Ensembles</a></h3><ul><li>Train multiple independent models, and at test time average their predictions. As the number of models in the ensemble increases, the performance typically monotonically improves (though with diminishing returns). The improvements are more dramatic with higher model variety in the ensemble.</li></ul><h2 id="cnns">CNNs</h2><ul><li>network will not learn duplicate filters because this is not OPTIMAL</li><li>A typical 2D convolution applied to an RGB image would have a filter shape of (3, filter_height, filter_width), so it combines information from all channels into a 2D output.</li><li>If you wanted to process each color separately (and equally), you would use a 3D convolution with filter shape (1, filter_height, filter_width).</li><li>conv-layers are compute intensive, dense layers are memory intensive</li><li>1D convolution is useful for data with local structure in one dimension, like audio or other time series.</li><li>Note that sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a ConvNet have some specific centered structure, where we should expect, for example, that completely different features should be learned on one side of the image than another.  One practical example is when the input are faces that have been centered in the image.  You might expect that different eye-specific or hair-specific features could (and should) be learned in different spatial locations.</li><li>In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a Locally-Connected Layer.</li></ul><h5 id="systematic-analysis-of-cnn-parameters">Systematic analysis of CNN parameters:</h5><p><a href="https://arxiv.org/pdf/1606.02228.pdf">https://arxiv.org/pdf/1606.02228.pdf</a></p><ul><li>use ELU non-linearity without batchnorm or ReLU with it.</li><li>apply a learned colorspace transformation of RGB.</li><li>use the linear learning rate decay policy.</li><li>use a sum of the average and max pooling layers.</li><li>use mini-batch size around 128 or 256. If this is too big for your GPU, decrease the learning rate proportionally to the batch size.</li><li>use fully-connected layers as convolutional and average the predictions for the final decision.</li><li>when investing in increasing training set size, check if a plateau has not been reach.</li><li>cleanliness of the data is more important then the size.</li><li>if you cannot increase the input image size, reduce the stride in the consequent layers, it has roughly the same effect.</li><li>if your network has a complex and highly optimized architecture, like e.g.  GoogLeNet, be careful with modifications.</li><li>maxpooling helps with translation invariance, helps find larger features, helpfull for any kind of convnets</li><li>maxpooling: you only care about the most “dogginess” of the picture, not the rest - better for pics where target is only small part of pic</li><li>averagepooling: you care about more of the entire picture not only the extremes - better for pics with one filling motive</li><li>Resnet has good regularization characteristics, authors do not include Dropout</li></ul><h2 id="rnns">RNNs</h2><h5 id="predict-multiple-steps">Predict multiple steps</h5><ol><li>“Function to Function Regression” which assumes that at the end of RNN, we are going to predict a curve. So use a multilayer perceptron at the end of RNN to predict multiple steps ahead.
Suppose you have a time series and you want to use its samples from 1, …, t to predict the ones in t+1, …, T. You use an RNN to learn a D dimensional representation for the first part of time series and then use a (D x (T-t)) MLP to forecast the second half of the time series. In practice, you do these two steps in a supervised way; i.e., you learn representations that improve the quality of the forecast.</li><li>tbd</li></ol><h3 id="lstm">LSTM</h3><p>The first dimension in Keras is the batch dimension. It can be any size, as long as it is the same for inputs and targets.
When dealing with LSTMs, the batch dimension is the number of sequences, not the length of the sequence.</p><ul><li>LSTMs in Keras are typically used on 3d data <strong><code>(batch dimension, timesteps, features)</code></strong>.</li><li>LSTM without return_sequences will output <strong><code>(batch dimension, output features)</code></strong></li><li>LSTM with return_sequences will output <strong><code>(batch dimension, timesteps, output features)</code></strong></li></ul><p>Basic timeseries data has an input shape (number of sequences, steps, features). Target is (number of sequences, steps, targets). Use an LSTM with return_sequences.</p><p><a href="http://stackoverflow.com/questions/43034960/many-to-one-and-many-to-many-lstm-examples-in-keras">stackoverflow</a></p><p><strong>One-to-one:</strong> equivalent to MLP.</p><pre><code>model.add(Dense(output_size, input_shape=input_shape))
</code></pre><p><strong>One-to-many:</strong> this option is not supported well, but this is a workaround:</p><pre><code>model.add(RepeatVector(number_of_times, input_shape=input_shape))
model.add(LSTM(output_size, return_sequences=True))
</code></pre><p><strong>Many-to-one:</strong>:</p><pre><code>model = Sequential()
model.add(LSTM(n, input_shape=(timesteps, data_dim)))
</code></pre><p><strong>Many-to-many:</strong> This is the easiest snippet when length of input and output matches the number of reccurent steps:</p><pre><code>model = Sequential()
model.add(LSTM(n, input_shape=(timesteps, data_dim), return_sequences=True))
</code></pre><p><strong>Many-to-many when number of steps differ from input/output length:</strong> this is hard in Keras. I did not find any code snippets to code that.</p><h4 id="tricks">Tricks</h4><ul><li>Within a batch, each sequence has its OWN states. Also each sequence is seen as independent from the other sequences in the batch. If your batch is Y, sequences Y[1] and Y[2] will never have something in common (whatever stateful=False or stateful=True).</li><li>If the LSTM has the stateful mode, the states of the current batch will be propagated to the next batch (at index i). Batching sequences is only to speed up the computations on a GPU.</li><li>3 weight matrices: input, recurring, output</li><li>fchollet: seq_len 100 seems large fora LSTM, Try 32</li><li>if there is one step lag between the actual time series, this is the most seen “trap” if you do time series prediction, in which the NN will always mimic previous input of time series. The function learned is only an one-step lag identity (mimic) prediction (trivial identity function).</li><li>The back propagation horizon is limited to the second dimension of the input sequence. i.e. if your data is of type (num_sequences, num_time_steps_per_seq, data_dim) then back prop is done over a time horizon of value num_time_steps_per_seq (<a href="https://github.com/fchollet/keras/issues/3669">https://github.com/fchollet/keras/issues/3669</a>)</li><li>all a stateful RNN does is remember the last activation. So if you have a large input sequence and break it up in smaller sequences, the activation in the network is retained in the network after processing the first sequence and therefore affects the activations in the network when processing the second sequence.</li><li>Keep all long term memory when modelling Time Series where you may have very long term memory and when you don’t know exactly when to cut. No subsampling here.</li><li>Recursive prediction of timesteps (multi-step) eventually uses values already predicted. This produces an accumulation of errors, which may grow very fast.</li></ul><h2 id="optimiziers">Optimiziers</h2><ul><li>better than adagrad: rmsprop, it does not explode, just jump around, when it flattens out you should device learning rate by 10–100</li><li>momentum + rmsprop = good idea -&gt; adam</li><li>SGD is well understood and a great place to start. ADAM is fast and gives good results and I often use it in practice.</li><li><a href="http://cs231n.github.io/neural-networks-3/#gradcheck">illustration</a> (Images credit: <a href="https://twitter.com/alecrad">Alec Radford</a>):<div class="document_logo__1FvRe"><img src="/static/media/opt2.5d5166a3.gif" class="document_logo-plant__sUnR_" alt="opt2"/></div><div class="document_logo__1FvRe"><img src="/static/media/opt1.4a3b4a39.gif" class="document_logo-plant__sUnR_" alt="opt1"/></div></li></ul><h5 id="momentum-09">Momentum (0.9)</h5><p>For NN’s,the hypersurface defined by our loss function often includes saddle points. These are areas where the gradient of the loss function often becomes very small in one or more axes, but there is no minima present. When the gradient is very small, this necessarily slows the gradient descent process down; this is of course what we desire when approaching a minima, but is detrimental otherwise. Momentum is intended to help speed the optimisation process through cases like this, to avoid getting stuck in these “shallow valleys”.</p><p>Momentum works by adding a new term to the update function, in addition to the gradient term. The added term can be thought of as the average of the previous gradients. Thus if the previous gradients were zig zagging through a saddle point, their average will be along the valley of the saddle point. Therefore, when we update our weights, we first move opposite the gradient. Then, we also move in the direction of the average of our last few gradients. This allows us to mitigate zig-zagging through valleys by forcing us along the average direction we’re zig-zagging towards.</p><h5 id="adagrad">Adagrad</h5><p>Adagrad is a technique that adjusts the learning rate for each individual parameter, based on the previous gradients for that parameter. Essentially, the idea is that if previous gradients were large, the new learning rate will be small, and vice versa.</p><p>The implementation looks at the gradients that were previously calculated for a parameter, then squares all of these gradients (which ignores the sign and only considers the magnitude), adds all of the squares together, and then takes the square root (otherwise known as the l2-norm). For the next epoch, the learning rate for this parameter is the overall learning rate divided by the l2-norm of prior updates. Therefore, if the l2-norm is large, the learning rate will be small; if it is small, the learning rate will be large.</p><p>Conceptually, this is a good idea. We know that typically, we want to our step sizes to be small when approaching minima. When they’re too large, we run the risk of bouncing out of minima. However there is no way for us to easily tell when we’re in a possible minima or not, so it’s difficult to recognize this situation and adjust accordingly. Adagrad attempts to do this by operating under the assumption that the larger the distance a parameter has traveled through optimization, the more likely it is to be near a minima; therefore, as the parameter covers larger distances, let’s decrease that parameter’s learning rate to make it more sensitive. That is the purpose of scaling the learning rate by the inverse of the l2-norm of that parameter’s prior gradients.</p><p>The one downfall to this assumption is that we may not actually have reached a minima by the time the learning rate is scaled appropriately. The l2-norm is always increasing, thus the learning rate is always decreasing. Because of this the training will reach a point where a given parameter can only ever be updated by a tiny amount, effectively meaning that parameter can no longer learn any further. This may or may not occur at an optimal range of values for that parameter.</p><p>Additionally, when updating millions of parameters, it becomes expensive to keep track of every gradient calculated in training, and then calculating the norm.</p><h5 id="rmsprop">RMSProp</h5><p>very similar to Adagrad, with the aim of resolving Adagrad’s primary limitation. Adagrad will continually shrink the learning rate for a given parameter (effectively stopping training on that parameter eventually). RMSProp however is able to shrink or increase the learning rate.</p><p>RMSProp will divide the overall learning rate by the square root of the sum of squares of the previous update gradients for a given parameter (as is done in Adagrad). The difference is that RMSProp doesn’t weight all of the previous update gradients equally, it uses an exponentially weighted moving average of the previous update gradients. This means that older values contribute less than newer values. This allows it to jump around the optimum without getting further and further away.</p><p>Further, it allows us to account for changes in the hypersurface as we travel down the gradient, and adjust learning rate accordingly. If our parameter is stuck in a shallow plain, we’d expect it’s recent gradients to be small, and therefore RMSProp increases our learning rate to push through it. Likewise, when we quickly descend a steep valley, RMSProp lowers the learning rate to avoid popping out of the minima.</p><h5 id="adam">Adam</h5><p>Adam (Adaptive Moment Estimation) combines the benefits of momentum with the benefits of RMSProp. Momentum is looking at the moving average of the gradient, and continues to adjust a parameter in that direction. RMSProp looks at the weighted moving average of the square of the gradients; this is essentially the recent variance in the parameter, and RMSProp shrinks the learning rate proportionally. Adam does both of these things - it multiplies the learning rate by the momentum, but also divides by a factor related to the variance.</p><h2 id="gotchas">Gotchas:</h2><ul><li>numpy matrix: rows by col, images: col by rows</li><li>weight conversion from Theano to Tensorflow: <a href="https://github.com/titu1994/Keras-Classification-Models/blob/master/weight_conversion_theano.py">https://github.com/titu1994/Keras-Classification-Models/blob/master/weight_conversion_theano.py</a></li></ul><h2 id="other">Other</h2><h3 id="problem-frameing">Problem Frameing</h3><h5 id="time-series">Time Series</h5><ul><li>LSTM not suited for <a href="https://machinelearningmastery.com/suitability-long-short-term-memory-networks-time-series-forecasting/">AR problems</a></li><li>MQTT realt time data: <a href="http://stackoverflow.com/questions/40652453/using-keras-for-real-time-training-and-predicting">http://stackoverflow.com/questions/40652453/using-keras-for-real-time-training-and-predicting</a></li><li>Apple stock: <a href="https://medium.com/machine-learning-world/neural-networks-for-algorithmic-trading-1-2-correct-time-series-forecasting-backtesting-9776bfd9e589">https://medium.com/machine-learning-world/neural-networks-for-algorithmic-trading-1–2-correct-time-series-forecasting-backtesting-9776bfd9e589</a></li></ul><h5 id="sentiment-analysis">Sentiment Analysis</h5><p>LTSM: input sequence -&gt; classification</p><h5 id="anomaly-detection">Anomaly Detection</h5><p>nietsche: come with a sequence and let it predict an hour into the future and look when it falls outside</p><h5 id="nlp">NLP:</h5><p>it is ordered data -&gt; 1D convolution
each word of our 5000 categories is converted in a vector of 32elements
model learns the 32 floats to be semantically significant
embeddings can be passed, not entire models (pretrained word embeddings)
word2vec (Google) vs. glove</p><h3 id="model-examples">Model Examples</h3><pre><code class="language-python" data-language="python" data-highlighted-line-numbers=""><span class="token comment">### Keras 2.0 Merge</span>
<span class="token comment"># Custom Merge: https://stackoverflow.com/questions/43160181/keras-merge-layer-warning</span>
<span class="token keyword">def</span> <span class="token function">euclid_dist</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>v<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> v<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>

<span class="token keyword">def</span> <span class="token function">out_shape</span><span class="token punctuation">(</span>shapes<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> shapes<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

merged_vector <span class="token operator">=</span> Lambda<span class="token punctuation">(</span>euclid_dist<span class="token punctuation">,</span> output_shape<span class="token operator">=</span>out_shape<span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">[</span>l1<span class="token punctuation">,</span> l2<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># https://github.com/fchollet/keras/issues/2299</span>
<span class="token comment"># http://web.cse.ohio-state.edu/~dwang/papers/Wang.tia08.pdf</span>
mix <span class="token operator">=</span> Input<span class="token punctuation">(</span>batch_shape<span class="token operator">=</span><span class="token punctuation">(</span>sequences<span class="token punctuation">,</span> timesteps<span class="token punctuation">,</span> features<span class="token punctuation">)</span><span class="token punctuation">)</span>
lstm <span class="token operator">=</span> LSTM<span class="token punctuation">(</span>features<span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span>LSTM<span class="token punctuation">(</span>features<span class="token punctuation">,</span> return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span>mix<span class="token punctuation">)</span><span class="token punctuation">)</span>
tdd1 <span class="token operator">=</span> TimeDistributed<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>features<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>lstm<span class="token punctuation">)</span>
tdd2 <span class="token operator">=</span> TimeDistributed<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>features<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>lstm<span class="token punctuation">)</span>
voice <span class="token operator">=</span> Lambda<span class="token punctuation">(</span>function<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> mask<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>merge<span class="token punctuation">(</span><span class="token punctuation">[</span>tdd1<span class="token punctuation">,</span> tdd2<span class="token punctuation">,</span> mix<span class="token punctuation">]</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'concat'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
background <span class="token operator">=</span> Lambda<span class="token punctuation">(</span>function<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> mask<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>merge<span class="token punctuation">(</span><span class="token punctuation">[</span>tdd2<span class="token punctuation">,</span> tdd1<span class="token punctuation">,</span> mix<span class="token punctuation">]</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'concat'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token punctuation">[</span>mix<span class="token punctuation">]</span><span class="token punctuation">,</span> output<span class="token operator">=</span><span class="token punctuation">[</span>voice<span class="token punctuation">,</span> background<span class="token punctuation">]</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>loss<span class="token operator">=</span><span class="token string">'mse'</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span><span class="token string">'rmsprop'</span><span class="token punctuation">)</span>

<span class="token comment">### Bidirectional RNN</span>
<span class="token comment"># https://github.com/fchollet/keras/issues/2838</span>
xin <span class="token operator">=</span> Input<span class="token punctuation">(</span>batch_shape<span class="token operator">=</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_size<span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'int32'</span><span class="token punctuation">)</span>
xemb <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>embedding_size<span class="token punctuation">,</span> mask_zero<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span>xin<span class="token punctuation">)</span>

rnn_fwd1 <span class="token operator">=</span> LSTM<span class="token punctuation">(</span>rnn_size<span class="token punctuation">,</span> return_sequence<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span>xemb<span class="token punctuation">)</span>
rnn_bwd1 <span class="token operator">=</span> LSTM<span class="token punctuation">(</span>rnn_size<span class="token punctuation">,</span> return_sequence<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> go_backwards<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">(</span>xemb<span class="token punctuation">)</span>
rnn_bidir1 <span class="token operator">=</span> merge<span class="token punctuation">(</span><span class="token punctuation">[</span>rnn_fwd1<span class="token punctuation">,</span> rnn_bwd1<span class="token punctuation">]</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'concat'</span><span class="token punctuation">)</span>

predictions <span class="token operator">=</span> TimeDistributed<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>output_class_size<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'softmax'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span>rnn_bidir1<span class="token punctuation">)</span> 

model <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>xin<span class="token punctuation">,</span> output<span class="token operator">=</span>predictions<span class="token punctuation">)</span>

<span class="token comment">### Multi Label Classification</span>
<span class="token comment"># Build a classifier optimized for maximizing f1_score (uses class_weights)</span>

clf <span class="token operator">=</span> Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>

clf<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
clf<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>xt<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1600</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
clf<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
clf<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">1600</span><span class="token punctuation">,</span> <span class="token number">1200</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
clf<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
clf<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">1200</span><span class="token punctuation">,</span> <span class="token number">800</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
clf<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
clf<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">800</span><span class="token punctuation">,</span> yt<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'sigmoid'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

clf<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span>Adam<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> loss<span class="token operator">=</span><span class="token string">'binary_crossentropy'</span><span class="token punctuation">)</span>

clf<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>xt<span class="token punctuation">,</span> yt<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> nb_epoch<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span> validation_data<span class="token operator">=</span><span class="token punctuation">(</span>xs<span class="token punctuation">,</span> ys<span class="token punctuation">)</span><span class="token punctuation">,</span> class_weight<span class="token operator">=</span>W<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

preds <span class="token operator">=</span> clf<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>

preds<span class="token punctuation">[</span>preds<span class="token operator">>=</span><span class="token number">0.5</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
preds<span class="token punctuation">[</span>preds<span class="token operator">&lt;</span><span class="token number">0.5</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>

<span class="token keyword">print</span> f1_score<span class="token punctuation">(</span>ys<span class="token punctuation">,</span> preds<span class="token punctuation">,</span> average<span class="token operator">=</span><span class="token string">'macro'</span><span class="token punctuation">)</span>
</code></pre><h3 id="principal-component-analysis-unsupervised">Principal Component Analysis (unsupervised)</h3><ul><li>selects the successive components that explain the maximum variance in the signal.</li></ul><pre><code class="language-python" data-language="python" data-highlighted-line-numbers="">    pca <span class="token operator">=</span> decomposition<span class="token punctuation">.</span>PCA<span class="token punctuation">(</span><span class="token punctuation">)</span>
    pca<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>pca<span class="token punctuation">.</span>explained_variance_<span class="token punctuation">)</span>

    <span class="token comment"># As we can see, only the 2 first components are useful</span>
    pca<span class="token punctuation">.</span>n_components <span class="token operator">=</span> <span class="token number">2</span>
    X_reduced <span class="token operator">=</span> pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    X_reduced<span class="token punctuation">.</span>shape
</code></pre><ul><li>In layman terms PCA helps to compress data and ICA helps to separate data.</li><li>PCA minimizes the covariance of the data; on the other hand ICA minimizes higher-order statistics such as fourth-order cummulant (or kurtosis), thus minimizing the mutual information of the output.</li><li>Specifically, PCA yields orthogonal vectors of high energy contents in terms of the variance of the signals, whereas</li><li>ICA identifies independent components for non-Gaussian signals. </li><li>In PCA the basis you want to find is the one that best explains the variability of your data. The first vector of the PCA basis is the one that best explains the variability of your data (the principal direction) the second vector is the 2nd best explanation and must be orthogonal to the first one, etc.</li><li>In ICA the basis you want to find is the one in which each vector is an independent component of your data, you can think of your data as a mix of signals and then the ICA basis will have a vector for each independent signal.</li><li>ICA will recover an orthogonal basis set of vectors</li></ul><h1 id="sources">Sources</h1><p><a href="http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-10.html">http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-10.html</a>
<a href="http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html">http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html</a>
<a href="http://course.fast.ai/">http://course.fast.ai/</a>
<a href="http://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network">http://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network</a></p><p>many more, which I do not remember…</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fishy Affine Transformation]]></title>
            <link>/posts/2017-03-13/fishy-affine-transformation</link>
            <guid>/posts/2017-03-13/fishy-affine-transformation</guid>
            <content:encoded><![CDATA[<h1 id="fishy-affine-transformation">Fishy Affine Transformation</h1><p>While working on the kaggle competition <a href="https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring">https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring</a> I hit the point when I wanted
to align fish based on an annotation at the fish’s head and tail, so that the fish is centered in the image, always in the same orientation
and distracting picture information is minimized. This required:</p><ol><li>finding the fish (thanks Nathaniel Shimoni for annotating)</li><li>centering</li><li>rotatating</li><li>cropping</li></ol><p>Mathematically the challenge is to find the associated  Affine Transformation. After years of working in a managerial role my linear algebra skills are a bit rusty so I decided to
invest the weekend.</p><h3 id="affine-transformation">Affine Transformation</h3><p><a href="http://mathworld.wolfram.com/AffineTransformation.html">Wolfram</a>: An affine transformation is any transformation that preserves collinearity (i.e., all points lying on a line initially still lie on a line after transformation) and ratios of distances (e.g., the midpoint of a line segment remains the midpoint after transformation).</p><p>I decided to use <a href="http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html#transformations">CV2</a> after hitting the wall with several other tools.
It was not the most convenient choice, but eventually it got me there. CV2 uses (2x3) transformation matrices for affine transformations so I had to adjust my 2d vectors accordingly.</p><p>The reason: Homogeneous Coordinates.</p><p>To combine rotation and translation in one operation one extra dimension is needed more than the model requires.
For planar things this is 3 components and for spatial things this is 4 components.
The operators take 3 components and return 3 components requiring 3x3 matrices.</p><p>Using vector algebra with numpy requires some extra consideration but is possible. Basically a (2,) matrix represented the 2-dim vectors. Small letters
denoted vector variables and caps matrices.</p><h2 id="1-finding-the-fish">1. Finding the Fish</h2><p>I used the annotations from labels produced by Nathaniel Shimoni and published on Kaggle (thanks for the great work!).</p><p>Using only fish with head and tail annotated, it was possible to get the vector representation of a fish as:</p><pre><code class="language-python" data-language="python" data-highlighted-line-numbers="">p_heads <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">(</span>img_data<span class="token punctuation">[</span><span class="token string">'annotations'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'x'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img_data<span class="token punctuation">[</span><span class="token string">'annotations'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'y'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
p_tails <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">(</span>img_data<span class="token punctuation">[</span><span class="token string">'annotations'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'x'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img_data<span class="token punctuation">[</span><span class="token string">'annotations'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'y'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
p_middle <span class="token operator">=</span> <span class="token punctuation">(</span>p_heads <span class="token operator">+</span> p_tails<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span>
v_fish <span class="token operator">=</span> p_heads <span class="token operator">-</span> p_tails
</code></pre><h2 id="2-centering">2. Centering</h2><p>Centering fish is a basic translation in the 2-dim space.</p><pre><code class="language-python" data-language="python" data-highlighted-line-numbers="">    <span class="token comment"># translate to center of img</span>
    img_center <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>img_height<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> img_width<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    t <span class="token operator">=</span> img_center <span class="token operator">-</span> p_middle  <span class="token comment"># translation vector</span>
    t <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>t<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># generate the 2x3 affine transformation matrix</span>
    T <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre><p>The respective transformation matrix is:</p><div class="document_logo__2nJ5O"><img src="/static/media/fishy-affine-transformation-translation.79a3cb79.png" class="document_logo-plant__EtAb-" alt="fishy-affine-transformation-translation"/></div><h2 id="3-rotating">3. Rotating</h2><p>First I needed to find the angle for rotation. I wanted to have the fish oriented parallel to the x-axis with the head always being on the right. The dot-product of two vectors provides the
angle in between, so I had to “dot-product” my fish vector with the x-axis:</p><pre><code class="language-python" data-language="python" data-highlighted-line-numbers=""><span class="token keyword">def</span> <span class="token function">unit_vector</span><span class="token punctuation">(</span>vector<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Returns the unit vector of the vector."""</span>
    <span class="token keyword">return</span> vector <span class="token operator">/</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>vector<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">angle_between</span><span class="token punctuation">(</span>v1<span class="token punctuation">,</span> v2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">""" Returns the angle in radians between vectors 'v1' and 'v2'::

            >>> angle_between((1, 0, 0), (0, 1, 0))
            1.5707963267948966
            >>> angle_between((1, 0, 0), (1, 0, 0))
            0.0
            >>> angle_between((1, 0, 0), (-1, 0, 0))
            3.141592653589793
    """</span>
    v1_u <span class="token operator">=</span> unit_vector<span class="token punctuation">(</span>v1<span class="token punctuation">)</span>
    v2_u <span class="token operator">=</span> unit_vector<span class="token punctuation">(</span>v2<span class="token punctuation">)</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>arccos<span class="token punctuation">(</span>np<span class="token punctuation">.</span>clip<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>v1_u<span class="token punctuation">,</span> v2_u<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

angle <span class="token operator">=</span> np<span class="token punctuation">.</span>rad2deg<span class="token punctuation">(</span>angle_between<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> v_fish<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><p>Conveniently CV2 provides a function to find the necessary transformation matrix (cv2.getRotationMatrix2D).</p><p>A challenge was to find out, that the rotation angle returned always is between 0–180°, so the following conditional differentiation was necessary
(rotation counter clockwise vs clockwise). It basically differentiates between the case that the head is above or below the tail:</p><pre><code class="language-python" data-language="python" data-highlighted-line-numbers="">    <span class="token comment"># get the Affine transformation matrix</span>
    <span class="token keyword">if</span> p_heads<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">></span> p_tails<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>  <span class="token comment"># head is above tail</span>
        M <span class="token operator">=</span> cv2<span class="token punctuation">.</span>getRotationMatrix2D<span class="token punctuation">(</span><span class="token punctuation">(</span>p_middle<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> p_middle<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> angle<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        M <span class="token operator">=</span> cv2<span class="token punctuation">.</span>getRotationMatrix2D<span class="token punctuation">(</span><span class="token punctuation">(</span>p_middle<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> p_middle<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span>angle<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
</code></pre><h2 id="putting-it-all-together">Putting it all together</h2><p>Getting the resulting transformation from a translation and rotation mathematically translates to a matrix product and applying the resulting
transformation matrix to the fish vector. To make the multiplication of a 2x3 tranlation matrix and a 2x3 rotation matrix possible the
following steps were necesary (combination of two affine transformations):</p><ul><li>allocate A1, A2, R matrices, all 3x3 identity matrices (eyes)</li><li>replace the top part of A1 and A2 with the transformation matrices T and M</li><li>get the resulting transformation (matrix product)</li><li>return the first two rows of R</li></ul><p>So RR was my final transformation matrix.</p><pre><code class="language-python" data-language="python" data-highlighted-line-numbers="">    <span class="token comment"># compinte affine transform: make them 3x3</span>
    <span class="token comment"># http://stackoverflow.com/questions/13557066/built-in-function-to-combine-affine-transforms-in-opencv</span>
    A1 <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
    A2 <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
    R <span class="token operator">=</span> np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
    A1<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> T
    A2<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> M
    R <span class="token operator">=</span> A1@A2
    RR <span class="token operator">=</span> R<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>
</code></pre><p>Getting the transformed image is now straightforward:</p><pre><code class="language-python" data-language="python" data-highlighted-line-numbers="">    dst <span class="token operator">=</span> cv2<span class="token punctuation">.</span>warpAffine<span class="token punctuation">(</span>img<span class="token punctuation">,</span> RR<span class="token punctuation">,</span> <span class="token punctuation">(</span>img_height<span class="token punctuation">,</span> img_width<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><p>The nice thing with this approach is that once you have got the final transformation matrix, all other points of interest can be transformed by this matrix,
e.g. the head and tail annotations are transformed by the same matrix.</p><h2 id="result">Result</h2><p>The blue point marks the head and the red point the tail. You can see the fish positioned arbitrarily in the image.
With the Affine Transformation the fish will be extracted and aligned.
The result is being displayed in the left upper corner.</p><div class="document_logo__2nJ5O"><img src="/static/media/fishy-affine-transformation-result.bfb7e795.png" class="document_logo-plant__EtAb-" alt="fishy-affine-transformation-result"/></div><p>With this technique I was able to align my fish and feed it into my machine learning models.</p><p>Thanks for reading.</p><h5 id="disclaimer">Disclaimer</h5><p>I use <a href="http://stackoverflow.com/">http://stackoverflow.com/</a> a lot. Not every source is quoted properly.<br/>
Other sources:<br/>
<a href="https://www.kaggle.com/qiubit/the-nature-conservancy-fisheries-monitoring/crop-fish">https://www.kaggle.com/qiubit/the-nature-conservancy-fisheries-monitoring/crop-fish</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Energy Forecast for a full scale Vehicle Plant]]></title>
            <link>/posts/2017-05-28/energy-prediction-lpz</link>
            <guid>/posts/2017-05-28/energy-prediction-lpz</guid>
            <content:encoded><![CDATA[<div class="document_logo__ebI6r"><img src="/static/media/plant_lpz.e9c11026.png" class="document_logo-plant__1ORN_" alt="Plant LPZ"/></div><h1 id="energy-forecast-for-a-full-scale-vehicle-plant">Energy Forecast for a full scale Vehicle Plant</h1><p>Energy forecasting is based on time series analysis.
There are many techniques for analysing and forecasting time series, e.g. ARIMA, linear regression and deep learning.
To tackle the challenge at hand a linear regression will be the benchmark model aganst which deep learning models will be tested. In particular a multi layer perceptron (MLP) and recurrent
neural network (RNN), i.e.  Long-Short Time Memory (LSTM) model will be applied.</p><h2 id="business-domain">Business Domain</h2><p>Energy forecasting is a tricky challenge because many factors might influence the final energy demand of a complex system
like a large manufacturing plant. Especially when the plant employs
not only energy consumers but also energy producers like CHPs and wind farms or energy reservoirs like battery farms.
Significant factors to take into account:</p><ul><li>production plan</li><li>CHP energy production</li><li>weather, i.e. temperature, wind</li></ul><h2 id="data-preparation">Data Preparation</h2><p>In order to apply the described techniques the problem has to be framed as a supervised learning problem. The data at hand is an hourly measurment of energy consumption
in 2015 as well as associated production plans and weather data. This results to a multivariate time series. The variable to forecast is energy consumption for the next 48 hours.</p><p>For application in a LSTM neural network with <strong><code>tanh</code></strong> non-linearity the data need to be scaled to the interval [-1,1]. Furthermore we split it into a training set (80%) and a test set (20%).</p><p>For forecasting different tactics can be applied.</p><ol><li><strong>Linear</strong> regression: the timesteps are taken as independent from the past an only dependent on the feature vector at time t=0.</li><li><strong>MLP</strong>: similar to linear regression with respect to feature preparation.</li><li><strong>LSTM</strong>: the timesteps are dependent on their predecessors and therefor the see-behind window is a hyperparameter to be chosen for the model.</li></ol><p>For this analysis the LSTM model will have two variants with regards to the lookback window:</p><ol><li>the entire dataset will be taken as sequence length, i.e. the LSTM context will be build over the entire time series. In Keras this results in a statfull LSTM network with batch-size 1 (online learning)..</li><li>a lookback window of 14days will be taken. This allows for batch-size &gt; 0 and a stateless LSTM network.</li></ol><p>For the non RNN models also information from previous timesteps can be encoded into the feature vector by just putting the values of past timesteps as additional features into the feature vector.
Here we also use the information of the last 14 days to be consistent within our model choices.</p><p>For all models the following parameters/features have been selected:</p><ol><li>energy consumption</li><li>air temperature</li><li>wind speed</li><li>wind direction</li><li>production plan</li></ol><p>This results in a feature vector for the linear models of dimension 1872:</p><ul><li>lookback: 14days<em>24h</em>5features</li><li>lookforward: 2days<em>24h</em>4features (5th parameter is the energy and is the label in our models to be forecasted)</li></ul><h4 id="timestamp-challenges">Timestamp Challenges</h4><p>Keeping the timestamps correct after all the data transformations is a special challenge which requires careful handling. The following diagram illustrates the topic. Left you can see the resulting dataset for a lookback window of 14days whereas on the right for a lookback window of 1hour. In order to compare results, the inverse date transformations have to take this into account.</p><div class="document_logo__ebI6r"><img src="/static/media/temporal_adjustment.a0f9a4a5.png" class="document_logo-plant__1ORN_" alt="Temporal Adjustment"/></div><h2 id="model">Model</h2><p>We predict the entire 48 hours with one prediction in order to avoid instabilities introduced by step-by-step forecasting and then using the forecast as feature for the next forecast.</p><p>For the linear regression the venerable <a href="http://scikit-learn.org/stable/">scikit-learn</a> library is used.
For all the deep-learning <a href="https://keras.io/">KERAS</a> and <a href="https://www.tensorflow.org/">TENSORFLOW</a> are the tools of choice.</p><div class="document_logo__ebI6r"><img src="/static/media/models.e98fca7a.png" class="document_logo-plant__1ORN_" alt="models"/></div><p>The MLP model has got 1.4 Mio parameters, so its capacity is much higher then the LSTM.
This gives already a first hint towards further optimization of model setup.</p><h2 id="result">Result</h2><p>Quality of forecast is measured as MSE (mean squared error). All plots show an arbitrary point in time of the test set with 14days
in the past and 2 days forecast. Every model is compared to the naive linear regression (red line).</p><h3 id="ltsm">LTSM</h3><div class="document_logo__ebI6r"><img src="/static/media/rnn.b799641a.png" class="document_logo-plant__1ORN_" alt="RNN"/></div><p>The LTSM model overall shows an MSE of 0.025 on the test set.</p><p>The red box shows an outlier in the linear regression.
It seems like the linear model did not pick up a significant feature like production plan properly.
The LSTM did a better job here.</p><h3 id="mlp">MLP</h3><div class="document_logo__ebI6r"><img src="/static/media/mlp.a3d4bbea.png" class="document_logo-plant__1ORN_" alt="MLP"/></div><p>The MLP model overall shows an MSE of 0.063 on the test set.</p><h3 id="conclusion">Conclusion</h3><p>Although both deep learning approaches can predict the shape of the time series well, the LSTM model exhibits higher accuracy.
Since the model capacity is much lower, this was a surprising outcome.</p><p>However both deep learning approaches seem struggle to match the quality of a simple linear regression forecast.
Due to time contraints no hyperparameter or model tuning has taken place. There are many areas for potential improvements, e.g.</p><ul><li>detailed feature preparation, especially production plans</li><li>exploration of more neural network configurations (number of layers, number of timesteps, number of neurons, …)</li><li>hyperparameter tuning (regularization, learning rates, optimiziers, …)</li></ul><p>Thanks for reading.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Munggoggo: A modern message based async agent framework]]></title>
            <link>/posts/2019-11-02/munggoggo</link>
            <guid>/posts/2019-11-02/munggoggo</guid>
            <content:encoded><![CDATA[<div class="document_Document__NswW2"><p>An asyncio based agent platform written in Python and based on RabbitMQ. </p><p>Agents are isolated processes which can host multiple parallel running behaviours implementing business
logic and can be deployed as independent kubernetes pods. Access to the agent mesh is via
REST/HTTP, websocket and jsonrpc.</p><p>A few lines of code implement a fully fledged agent with a simple behaviour: sending and
receiving ping messages to/from other agents.</p><pre><code class="language-python" data-language="python" data-highlighted-line-numbers=""><span class="token keyword">from</span> mode <span class="token keyword">import</span> Worker
<span class="token keyword">from</span> behaviour <span class="token keyword">import</span> Behaviour
<span class="token keyword">from</span> core <span class="token keyword">import</span> Core


<span class="token keyword">class</span> <span class="token class-name">Agent</span><span class="token punctuation">(</span>Core<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">class</span> <span class="token class-name">PingBehav</span><span class="token punctuation">(</span>Behaviour<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">setup</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>counter <span class="token operator">=</span> <span class="token number">0</span>

        <span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">run</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>counter <span class="token operator">+=</span> <span class="token number">1</span>
            msg <span class="token operator">=</span> <span class="token keyword">await</span> self<span class="token punctuation">.</span>receive<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> msg<span class="token punctuation">:</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>name<span class="token punctuation">}</span></span><span class="token string">: Message received: </span><span class="token interpolation"><span class="token punctuation">{</span>msg<span class="token punctuation">.</span>body<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
            <span class="token keyword">await</span> self<span class="token punctuation">.</span>publish<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>counter<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'ping'</span><span class="token punctuation">)</span>
            <span class="token keyword">await</span> asyncio<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">)</span>

    <span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">setup</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">""" Register behaviour and subscribe to 'ping' topic """</span>
        <span class="token keyword">await</span> self<span class="token punctuation">.</span>add_runtime_dependency<span class="token punctuation">(</span>self<span class="token punctuation">.</span>PingBehav<span class="token punctuation">(</span>self<span class="token punctuation">,</span> binding_keys<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'ping'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    Worker<span class="token punctuation">(</span>Agent<span class="token punctuation">(</span>identity<span class="token operator">=</span><span class="token string">'AgentIdentity'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> loglevel<span class="token operator">=</span><span class="token string">"info"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>execute_from_commandline<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><p>Documentation: <a href="https://munggoggo.readthedocs.io/en/latest/">https://munggoggo.readthedocs.io/en/latest/</a><br/>
Github: <a href="https://github.com/sysid/munggoggo">https://github.com/sysid/munggoggo</a></p><h2 id="why">Why</h2><p>The physical world forms a massive parallel system.  </p><p>The metaphor of software agents communicating via messages helps to model this realty.
Devices like cars, buildings, etc. can be interpreted as independently acting
agents communicating via message protocols like AMQP. </p><p>Today’s software technology has evolved far enough to model this reality without access to supercomputers or
highly specialized programming paradigms. Agents running async business logic are a suitable fit and
can be implemented with various prevalent technology stacks.</p><h2 id="technology-stack-choice">Technology Stack Choice</h2><p>Requirements for our tech stack, driven by business reality:</p><ol><li>very fast turnover and implementation cycles (experiment, learn, extract useful part, repeat…)</li><li>focus on data driven models and algorithms, based on IoT data streams (e.g. vehicles)</li><li>loose coupling and independent entity modelling </li><li>cloud first (of course…)</li><li>standards based and seamless systems integration</li><li>scalability</li></ol><p>For us this translated in:</p><ol><li>Python as programming language wich allows quick development cycles and has got a well established footprint
in data science and data computing</li><li>Messaging backend based on AMQP (RabbitMQ) which supports streaming processing while simultaneously allowing
sophisticated routing and communication patterns</li><li>Async programming paradigm in order to fully capitalize on stream processing</li><li>Agent metaphor for independent computing units exposing one ore more behaviours</li></ol><h2 id="bringing-it-together">Bringing it together</h2><p>Combining these ideas, reviewing existing solutions and including a few original ideas
eventually led to this prototype implementation of a behavioural agent software framework based on async python, RabbitMQ and
ASGI as web programming model.  </p><p>It is a personal PoC rather than production ready software and its potential is to be verified.</p><p>In any case it demonstrates some interesting ideas to address the described problem domain and combines some of the latest
python programming paradigms.</p></div>]]></content:encoded>
        </item>
    </channel>
</rss>