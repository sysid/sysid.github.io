(window.webpackJsonp=window.webpackJsonp||[]).push([[3],{59:function(e,t,a){"use strict";a.r(t),a.d(t,"readingTime",function(){return m}),a.d(t,"default",function(){return j}),a.d(t,"tableOfContents",function(){return O}),a.d(t,"frontMatter",function(){return w});var n=a(14),r=(a(0),a(20)),o=a(60),i=a.n(o),l=a(61),s=a.n(l),c=a(62),p=a.n(c),b=a(63),d=a.n(b),h=a(64),u=a.n(h),m={text:"5 min read",minutes:4.52,time:271200,words:904},g={},f="wrapper";function j(e){var t=e.components,a=Object(n.a)(e,["components"]);return Object(r.b)(f,Object.assign({},g,a,{components:t,mdxType:"MDXLayout"}),Object(r.b)("div",{className:u.a.logo},Object(r.b)("img",{src:i.a,className:u.a["logo-plant"],alt:"Plant LPZ"})),Object(r.b)("h1",{id:"energy-forecast-for-a-full-scale-vehicle-plant"},"Energy Forecast for a full scale Vehicle Plant"),Object(r.b)("p",null,"Energy forecasting is based on time series analysis.\nThere are many techniques for analysing and forecasting time series, e.g. ARIMA, linear regression and deep learning.\nTo tackle the challenge at hand a linear regression will be the benchmark model aganst which deep learning models will be tested. In particular a multi layer perceptron (MLP) and recurrent\nneural network (RNN), i.e.  Long-Short Time Memory (LSTM) model will be applied."),Object(r.b)("h2",{id:"business-domain"},"Business Domain"),Object(r.b)("p",null,"Energy forecasting is a tricky challenge because many factors might influence the final energy demand of a complex system\nlike a large manufacturing plant. Especially when the plant employs\nnot only energy consumers but also energy producers like CHPs and wind farms or energy reservoirs like battery farms.\nSignificant factors to take into account:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"production plan"),Object(r.b)("li",{parentName:"ul"},"CHP energy production"),Object(r.b)("li",{parentName:"ul"},"weather, i.e. temperature, wind")),Object(r.b)("h2",{id:"data-preparation"},"Data Preparation"),Object(r.b)("p",null,"In order to apply the described techniques the problem has to be framed as a supervised learning problem. The data at hand is an hourly measurment of energy consumption\nin 2015 as well as associated production plans and weather data. This results to a multivariate time series. The variable to forecast is energy consumption for the next 48 hours."),Object(r.b)("p",null,"For application in a LSTM neural network with ",Object(r.b)("strong",{parentName:"p"},Object(r.b)("inlineCode",{parentName:"strong"},"tanh"))," non-linearity the data need to be scaled to the interval ","[-1,1]",". Furthermore we split it into a training set (80%) and a test set (20%)."),Object(r.b)("p",null,"For forecasting different tactics can be applied."),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},Object(r.b)("strong",{parentName:"li"},"Linear")," regression: the timesteps are taken as independent from the past an only dependent on the feature vector at time t=0."),Object(r.b)("li",{parentName:"ol"},Object(r.b)("strong",{parentName:"li"},"MLP"),": similar to linear regression with respect to feature preparation."),Object(r.b)("li",{parentName:"ol"},Object(r.b)("strong",{parentName:"li"},"LSTM"),": the timesteps are dependent on their predecessors and therefor the see-behind window is a hyperparameter to be chosen for the model.")),Object(r.b)("p",null,"For this analysis the LSTM model will have two variants with regards to the lookback window:"),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},"the entire dataset will be taken as sequence length, i.e. the LSTM context will be build over the entire time series. In Keras this results in a statfull LSTM network with batch-size 1 (online learning).."),Object(r.b)("li",{parentName:"ol"},"a lookback window of 14days will be taken. This allows for batch-size > 0 and a stateless LSTM network.")),Object(r.b)("p",null,"For the non RNN models also information from previous timesteps can be encoded into the feature vector by just putting the values of past timesteps as additional features into the feature vector.\nHere we also use the information of the last 14 days to be consistent within our model choices."),Object(r.b)("p",null,"For all models the following parameters/features have been selected:"),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},"energy consumption"),Object(r.b)("li",{parentName:"ol"},"air temperature"),Object(r.b)("li",{parentName:"ol"},"wind speed"),Object(r.b)("li",{parentName:"ol"},"wind direction"),Object(r.b)("li",{parentName:"ol"},"production plan")),Object(r.b)("p",null,"This results in a feature vector for the linear models of dimension 1872:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"lookback: 14days",Object(r.b)("em",{parentName:"li"},"24h"),"5features"),Object(r.b)("li",{parentName:"ul"},"lookforward: 2days",Object(r.b)("em",{parentName:"li"},"24h"),"4features (5th parameter is the energy and is the label in our models to be forecasted)")),Object(r.b)("h4",{id:"timestamp-challenges"},"Timestamp Challenges"),Object(r.b)("p",null,"Keeping the timestamps correct after all the data transformations is a special challenge which requires careful handling. The following diagram illustrates the topic. Left you can see the resulting dataset for a lookback window of 14days whereas on the right for a lookback window of 1hour. In order to compare results, the inverse date transformations have to take this into account."),Object(r.b)("div",{className:u.a.logo},Object(r.b)("img",{src:s.a,className:u.a["logo-plant"],alt:"Temporal Adjustment"})),Object(r.b)("h2",{id:"model"},"Model"),Object(r.b)("p",null,"We predict the entire 48 hours with one prediction in order to avoid instabilities introduced by step-by-step forecasting and then using the forecast as feature for the next forecast."),Object(r.b)("p",null,"For the linear regression the venerable ",Object(r.b)("a",Object.assign({parentName:"p"},{href:"http://scikit-learn.org/stable/"}),"scikit-learn")," library is used.\nFor all the deep-learning ",Object(r.b)("a",Object.assign({parentName:"p"},{href:"https://keras.io/"}),"KERAS")," and ",Object(r.b)("a",Object.assign({parentName:"p"},{href:"https://www.tensorflow.org/"}),"TENSORFLOW")," are the tools of choice."),Object(r.b)("p",null,'{{< figure src="/images/prognose/models.png" title="" >}}'),Object(r.b)("p",null,"The MLP model has got 1.4 Mio parameters, so its capacity is much higher then the LSTM.\nThis gives already a first hint towards further optimization of model setup."),Object(r.b)("h2",{id:"result"},"Result"),Object(r.b)("p",null,"Quality of forecast is measured as MSE (mean squared error). All plots show an arbitrary point in time of the test set with 14days\nin the past and 2 days forecast. Every model is compared to the naive linear regression (red line)."),Object(r.b)("h3",{id:"ltsm"},"LTSM"),Object(r.b)("div",{className:u.a.logo},Object(r.b)("img",{src:d.a,className:u.a["logo-plant"],alt:"RNN"})),Object(r.b)("p",null,"The LTSM model overall shows an MSE of 0.025 on the test set."),Object(r.b)("p",null,"The red box shows an outlier in the linear regression.\nIt seems like the linear model did not pick up a significant feature like production plan properly.\nThe LSTM did a better job here."),Object(r.b)("h3",{id:"mlp"},"MLP"),Object(r.b)("div",{className:u.a.logo},Object(r.b)("img",{src:p.a,className:u.a["logo-plant"],alt:"MLP"})),Object(r.b)("p",null,"The MLP model overall shows an MSE of 0.063 on the test set."),Object(r.b)("h3",{id:"conclusion"},"Conclusion"),Object(r.b)("p",null,"Although both deep learning approaches can predict the shape of the time series well, the LSTM model exhibits higher accuracy.\nSince the model capacity is much lower, this was a surprising outcome."),Object(r.b)("p",null,"However both deep learning approaches seem struggle to match the quality of a simple linear regression forecast.\nDue to time contraints no hyperparameter or model tuning has taken place. There are many areas for potential improvements, e.g."),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"detailed feature preparation, especially production plans"),Object(r.b)("li",{parentName:"ul"},"exploration of more neural network configurations (number of layers, number of timesteps, number of neurons, \u2026)"),Object(r.b)("li",{parentName:"ul"},"hyperparameter tuning (regularization, learning rates, optimiziers, \u2026)")),Object(r.b)("p",null,"Thanks for reading."))}j.isMDXComponent=!0;var O=function(){arguments.length>0&&void 0!==arguments[0]&&arguments[0];return[{id:"business-domain",level:2,title:"Business Domain",children:[]},{id:"data-preparation",level:2,title:"Data Preparation",children:[]},{id:"model",level:2,title:"Model",children:[]},{id:"result",level:2,title:"Result",children:[{id:"ltsm",level:3,title:"LTSM",children:[]},{id:"mlp",level:3,title:"MLP",children:[]},{id:"conclusion",level:3,title:"Conclusion",children:[]}]}]},w={}},60:function(e,t,a){e.exports=a.p+"static/media/plant_lpz.e9c11026.png"},61:function(e,t,a){e.exports=a.p+"static/media/temporal_adjustment.a0f9a4a5.png"},62:function(e,t,a){e.exports=a.p+"static/media/mlp.a3d4bbea.png"},63:function(e,t,a){e.exports=a.p+"static/media/rnn.b799641a.png"},64:function(e,t,a){e.exports={logo:"document_logo__ebI6r","logo-navi":"document_logo-navi__2cHsD","logo-react":"document_logo-react__JhkE5","logo-plant":"document_logo-plant__1ORN_","Index-logo-react-spin":"document_Index-logo-react-spin__1ufuE","Index-logo-navi-spin":"document_Index-logo-navi-spin__2ubZJ"}}}]);
//# sourceMappingURL=3.4657ce02.chunk.js.map