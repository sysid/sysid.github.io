<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ml on </title>
    <link>https://sysid.github.io/tags/ml/</link>
    <description>Recent content in Ml on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>All rights reserved - 2016</copyright>
    <lastBuildDate>Tue, 07 Mar 2017 21:53:53 +0100</lastBuildDate>
    <atom:link href="https://sysid.github.io/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>energy consumption prediction with prophet</title>
      <link>https://sysid.github.io/post/energy-consumption-prediction-with-prophet/</link>
      <pubDate>Tue, 07 Mar 2017 21:53:53 +0100</pubDate>
      
      <guid>https://sysid.github.io/post/energy-consumption-prediction-with-prophet/</guid>
      <description>

&lt;h1 id=&#34;energy-consumption-prediction-with-facebook-s-prophet-library&#34;&gt;Energy Consumption Prediction with Facebook&amp;rsquo;s Prophet library&lt;/h1&gt;

&lt;p&gt;I already introduced &lt;a href=&#34;https://facebookincubator.github.io/prophet/&#34;&gt;Prophet&lt;/a&gt; as a time-series prediction tool which has a
exceptionally well designed user interface and is easy to use for simple cases while delivering impressive results
(&lt;a href=&#34;https://sysid.github.io/post/be-a-prophet-for-airline-data/&#34;&gt;see airline use case&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Today I will use &lt;a href=&#34;https://facebookincubator.github.io/prophet/&#34;&gt;Prophet&lt;/a&gt; for predicting energy consumption in Germany based on real data. Recently I covered the &lt;a href=&#34;https://sysid.github.io/post/energy-consumption-prediction/&#34;&gt;same use
case with Recurrent Neural Networks&lt;/a&gt; so this will serve as comparison of two different prediction approaches.&lt;/p&gt;

&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;

&lt;p&gt;Raw data is the hourly energy consumption of Germany from 2014 to end of 2016 and then further predicted up until 2018.
This timeseries from 2014-2018 will then in total be used as basis for predition into 2019.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumptionProphet/consumptionProphet1.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;

&lt;p&gt;Again it is surprising how user friendly Facebook designed the library interface. You don&amp;rsquo;t really need to have any
programming skills in order to use most of the framework out of the box:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m = Prophet()
m.fit(de)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;prediction&#34;&gt;Prediction&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;days = 365
future = m.make_future_dataframe(periods=24*days, freq=&#39;H&#39;)
forecast = m.predict(future)
m.plot(forecast)
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumptionProphet/consumptionProphet2.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;You can see high variance in the forecast due to day and night differences of 40000 GW. It would be a straightforward exercise
to narrow the variance by eliminating day-night differences and only look at averages or top figures.&lt;/p&gt;

&lt;p&gt;When looking at the components of the intrapolated curve you can nicely identify the trends in energy consumption on weekends
as well as during summer months and the christmas break. Also the overall decreasing trend in power consumption has been picked up correctly. The little peak in 2016 does not seem to be statically significant. However, this probably needs further investigation.&lt;/p&gt;

&lt;p&gt;The nice thing is: Prophet gives you this decomposition &amp;lsquo;for free&amp;rsquo;.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumptionProphet/consumptionProphet3.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;Not only fits the model the real seasonal trends quite well, it extrapoloates them into the future as prediction. The quality of
the prediction has not been assessed, but in this analysis I am only interested in picking up the time dependencies and trends over an extended period.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Compared to to time series prediction with Recurrent Neuronal Networks the approach is much simpler and time effective. At least
for this particular well behaved use case the results are good in both cases.&lt;/p&gt;

&lt;p&gt;Certainly this paper is only a very rough qualitätive proof of concept, but it is promising enough to incorporate  &lt;a href=&#34;https://facebookincubator.github.io/prophet/&#34;&gt;Prophet&lt;/a&gt; into my toolbelt for time series forecasting.&lt;/p&gt;

&lt;p&gt;Thanks for reading.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>energy consumption prediction</title>
      <link>https://sysid.github.io/post/energy-consumption-prediction/</link>
      <pubDate>Sun, 05 Mar 2017 19:39:50 +0100</pubDate>
      
      <guid>https://sysid.github.io/post/energy-consumption-prediction/</guid>
      <description>

&lt;h1 id=&#34;energy-consumption-prediction-with-recurrent-neural-networks&#34;&gt;Energy Consumption Prediction with Recurrent Neural Networks&lt;/h1&gt;

&lt;p&gt;We look at 52 European energy markets and their power consumption patterns.&lt;/p&gt;

&lt;p&gt;The forecast technology will be based on recurrent neural networks.
While there are many other, also simpler techniques (e.g. ARIMA, Facebook&amp;rsquo;s Prophet, time-shifting) this is an attempt to harness machine learning capabilities to predict future consumption patterns.&lt;/p&gt;

&lt;p&gt;The data consists of hourly values of country/region power consumption of 52 countries over the course from 1.1.2014 - 31.12.2018,
which gives us 43000 data points per country. Of course the data consists only of
real world data up to 2016 and then forcast data from a commercial data provider up until end of 2018.&lt;/p&gt;

&lt;p&gt;For our purposes we assume the entire time series as real data and try to predict time intervals with machine learning techniques.&lt;/p&gt;

&lt;h4 id=&#34;energy-consumption-germany&#34;&gt;Energy Consumption Germany&lt;/h4&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/consumption_DE.png&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;br&gt;
The data is generally quite regular and periodic, so this seems to be an adequate use case for RNN prediction.&lt;/p&gt;

&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;

&lt;p&gt;The problem we are facing is Multivariate time series regression.&lt;/p&gt;

&lt;p&gt;In order to run the code, we make sure that the data matrix is a numpy array of size
&lt;strong&gt;&lt;code&gt;T x N&lt;/code&gt;&lt;/strong&gt; where &lt;strong&gt;&lt;code&gt;T&lt;/code&gt;&lt;/strong&gt; is the length of time series and &lt;strong&gt;&lt;code&gt;N&lt;/code&gt;&lt;/strong&gt; is the number of them.&lt;/p&gt;

&lt;p&gt;Technically, with RNNs we should pass the entire history of time series and RNNs should be able to capture
the patterns over long period of time.&lt;/p&gt;

&lt;p&gt;Another approach is to break long sequences into smaller pieces. The size of window determines the longest
patterns in time that can possibly be captured by RNN. You need to select the window size according to the
expected length of temporal patterns.&lt;/p&gt;

&lt;p&gt;For our analysis we set the period length at 80 datapoints which is equivalent to an 80hour time forecast.&lt;/p&gt;

&lt;p&gt;So the entire &amp;lsquo;history&amp;rsquo; for our RNN consists of 80 hours of energy consumption.&lt;/p&gt;

&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;

&lt;p&gt;I tested three different models based on Gated Recurrent Units (GRU) as RNN units.&lt;/p&gt;

&lt;p&gt;The dataset, a &lt;strong&gt;&lt;code&gt;43000x52&lt;/code&gt;&lt;/strong&gt; matrix, was sequenced into a &lt;strong&gt;&lt;code&gt;464x80x52&lt;/code&gt;&lt;/strong&gt; tensor for training and a &lt;strong&gt;&lt;code&gt;82x80x52&lt;/code&gt;&lt;/strong&gt; tensor for testing.
This gives us 82 potential time periods of 80 hours per market for testing.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;&lt;code&gt;464x80x52&lt;/code&gt;&lt;/strong&gt; tensor is equivalent to 464 different timeseries per market, each of length 80.&lt;/p&gt;

&lt;p&gt;The model has been trained on the training set with &lt;strong&gt;&lt;code&gt;100&lt;/code&gt;&lt;/strong&gt; epochs and a &lt;strong&gt;&lt;code&gt;batch_size of 64&lt;/code&gt;&lt;/strong&gt;. Training took about 2 minutes per model on a NVIDIA GTX780.&lt;/p&gt;

&lt;p&gt;To test the accuracy I will here only present a visual comparision between the predicted curve and the real data. To this end a time sequence from the test set
and a market has been chosen randomly and plotted.&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;h3 id=&#34;model-1&#34;&gt;Model 1&lt;/h3&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/model1.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/model1_i10_m08.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;h3 id=&#34;model-2&#34;&gt;Model 2&lt;/h3&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/model2.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/model2_i27_m50.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;h3 id=&#34;model-3&#34;&gt;Model 3&lt;/h3&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/model3.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/consumption/model3_i46_m50.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;I did not bother to retransform the axis scales to the original units MW and Date because here I am only interested in a qualitative asssessment of
forecast quality.&lt;/p&gt;

&lt;p&gt;Surprising for me was to see, that all three models more or less deliver similar good results in projecting the time dependency of the energy consumption.&lt;/p&gt;

&lt;p&gt;This is remarkable since there has been no hyperparamter tuning at all. The models were fit &amp;lsquo;out of the box&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;Since this result has been achieved with very little work, using RNNs for this kind of time series prediction seems to be effective.&lt;/p&gt;

&lt;p&gt;Thanks for reading.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Facebook&#39;s Prophet forecasting library</title>
      <link>https://sysid.github.io/post/be-a-prophet-for-airline-data/</link>
      <pubDate>Sun, 05 Mar 2017 17:16:36 +0100</pubDate>
      
      <guid>https://sysid.github.io/post/be-a-prophet-for-airline-data/</guid>
      <description>

&lt;h1 id=&#34;be-a-prophet-for-1960-s-airline-data&#34;&gt;Be a prophet for 1960&amp;rsquo;s airline data&lt;/h1&gt;

&lt;p&gt;Facebook released a forecasting library for business timeseries, called &lt;a href=&#34;https://facebookincubator.github.io/prophet/&#34;&gt;Prophet&lt;/a&gt;. It&amp;rsquo;s designed
to work similar to the phantastic &lt;a href=&#34;phttp://scikit-learn.org/stable/&#34;&gt;scikit-learn&lt;/a&gt; library and is therefor easy to use.&lt;/p&gt;

&lt;p&gt;In order to give an example of its usefulness and effectiveness I will apply it to the good old airline passenger dataset.&lt;/p&gt;

&lt;p&gt;The input to &lt;strong&gt;Prophet&lt;/strong&gt; is always a dataframe with two columns: &lt;strong&gt;&lt;code&gt;ds&lt;/code&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;code&gt;y&lt;/code&gt;&lt;/strong&gt;. The &lt;strong&gt;&lt;code&gt;ds (datestamp)&lt;/code&gt;&lt;/strong&gt; column must contain a date or datetime (either is fine). The &lt;strong&gt;&lt;code&gt;y&lt;/code&gt;&lt;/strong&gt; column must be numeric, and represents the measurement we wish to forecast.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(DATA_HOME_DIR+&#39;/international-airline-passengers.csv&#39;,
                 sep=&#39;;&#39;,
                 names=[&#39;ds&#39;, &#39;y&#39;],
                 header=0,
                 parse_dates=[0],
                 nrows=144,
                )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since the data seems to incorporate an exponential growth trend, we preprocess it by taking the &lt;strong&gt;&lt;code&gt;log&lt;/code&gt;&lt;/strong&gt; and use the linear
trend fitting capabilities of &lt;strong&gt;Prophet&lt;/strong&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;y&#39;] = np.log(df[&#39;y&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We only have monthly data, so certainly there will be no weekly seasonality in the date. Forecasting must take this into account and choose the right frequency for the target dates.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m = Prophet(weekly_seasonality=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can simply call the &lt;strong&gt;&lt;code&gt;fit&lt;/code&gt;&lt;/strong&gt; method in order to fit the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;m.fit(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Predictions are then made on a dataframe with a column &lt;strong&gt;&lt;code&gt;ds&lt;/code&gt;&lt;/strong&gt; containing the dates for which a prediction is to be made. You can get a suitable dataframe that extends into the future a specified number of days using the helper method &lt;strong&gt;&lt;code&gt;Prophet.make_future_dataframe&lt;/code&gt;&lt;/strong&gt;. By default it will also include the dates from the history, so we will see the model fit as well.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;future = m.make_future_dataframe(periods=36, freq=&#39;M&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;&lt;code&gt;predict&lt;/code&gt;&lt;/strong&gt; method will assign each row in &lt;strong&gt;&lt;code&gt;future&lt;/code&gt;&lt;/strong&gt; a predicted value which it names yhat. If you pass in historical dates, it will provide an in-sample fit. The forecast object here is a new dataframe that includes a column yhat with the forecast, as well as columns for components and uncertainty intervals.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;forecast = m.predict(future)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The result is promising:

&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/airline_fit.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Prophet automatically gives you an overall trend analysis and decomposes the time series into its constituing compontents like yearly seasonality:

&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/airline_comp1.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Seeing a almost perfect linear trend confirms the original hypothesis of having a exponential growth trend.&lt;/p&gt;

&lt;p&gt;The steps in the yearly trend plot seem to be an artefact of fitting. Having a peak in July for travel numbers seem to indicate  the holiday season in the western hemisphere. However this is just an asumption since I have not been born then and are not familiar of the holiday season arrangments of the time.&lt;/p&gt;

&lt;p&gt;If you are interested in another helpful example, I refer to the excellent article of Arne: &lt;a href=&#34;https://arnesund.com/2017/02/26/using-facebook-prophet-forecasting-library-to-predict-the-weather/&#34;&gt;https://arnesund.com/2017/02/26/using-facebook-prophet-forecasting-library-to-predict-the-weather/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LSTM versus MLP for timeseries</title>
      <link>https://sysid.github.io/post/adding-problem/</link>
      <pubDate>Mon, 20 Feb 2017 07:23:42 +0100</pubDate>
      
      <guid>https://sysid.github.io/post/adding-problem/</guid>
      <description>

&lt;p&gt;Long Short Term Memory neural networks versus Multi Layer Perceptrons for time series:&lt;/p&gt;

&lt;p&gt;Playing around with RNN and LSTM for time series modelling so far resulted in disappointment. Traditional MLPs seem to perform better.
On the internet RNNs are often recommended for time-series data, but my results do not confirm this sentiment. Published examples on the internet normaly do not include a comparision
with MLP models, so I decided to analyse performance of LSTM time-series forecasting versus MLP systematically.&lt;/p&gt;

&lt;p&gt;First I needed to decided on the experimental setup and I started with a dataset which migth not seem as the natural first choice for timeseries examples: The Adding Problem.&lt;/p&gt;

&lt;p&gt;My inspiration herefor was &lt;a href=&#34;http://simaaron.github.io/Estimating-rainfall-from-weather-radar-readings-using-recurrent-neural-networks/&#34;&gt;http://simaaron.github.io/Estimating-rainfall-from-weather-radar-readings-using-recurrent-neural-networks/&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;part-1-the-adding-problem&#34;&gt;Part 1: The Adding Problem&lt;/h3&gt;

&lt;p&gt;The prediction of cumulative values from variable-length sequences of vectors with a ‘time’ component is highly reminiscent of the so-called
&lt;em&gt;Adding Problem&lt;/em&gt;
 in machine learning—a toy sequence regression task that is designed to demonstrate the power of recurrent neural networks (RNN) in learning long-term dependencies (see
 &lt;a href=&#34;http://arxiv.org/abs/1504.00941&#34;&gt;Le et al.&lt;/a&gt;
 , Sec. 4.1, for a recent example):&lt;/p&gt;

&lt;!-- 
&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/RNN_adding.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;RNN&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
 --&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://sysid.github.io/images/RNN_adding.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;&lt;br&gt;
Braced with this data I started my experiments to compare LSTM with MLP &lt;a href=&#34;../../nbs/adding.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Traditional MLP models seem to yield better results for this kind of sequence problem. They converge for sequence sizes &amp;gt; 50 and seem to have lower MSE.
This leaves me wondering whether the prevalent opinion on the internet on RNNs and especially LSTMs for time series data modelling seems to be misguided.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>